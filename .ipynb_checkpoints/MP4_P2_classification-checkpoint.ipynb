{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import string\n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from rnn.helpers import time_since\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language recognition with an RNN\n",
    "\n",
    "If you've ever used an online translator you've probably seen a feature that automatically detects the input language. While this might be easy to do if you input unicode characters that are unique to one or a small group of languages (like \"你好\" or \"γεια σας\"), this problem is more challenging if the input only uses the available ASCII characters. In this case, something like \"těší mě\" would beome \"tesi me\" in the ascii form. This is a more challenging problem in which the language must be recognized purely by the pattern of characters rather than unique unicode characters.\n",
    "\n",
    "We will train an RNN to solve this problem for a small set of languages thta can be converted to romanized ASCII form. For training data it would be ideal to have a large and varied dataset in different language styles. However, it is easy to find copies of the Bible which is a large text translated to different languages but in the same easily parsable format, so we will use 20 different copies of the Bible as training data. Using the same book for all of the different languages will hopefully prevent minor overfitting that might arise if we used different books for each language (fitting to common characteristics of the individual books rather than the language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesi me\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode as unicodeToAscii\n",
    "\n",
    "all_characters = string.printable\n",
    "n_letters = len(all_characters)\n",
    "\n",
    "print(unicodeToAscii('těší mě'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a file and split into lines\n",
    "def readFile(filename):\n",
    "    data = open(filename, encoding='utf-8').read().strip()\n",
    "    return unicodeToAscii(data)\n",
    "\n",
    "def get_category_data(data_path):\n",
    "    # Build the category_data dictionary, a list of names per language\n",
    "    category_data = {}\n",
    "    all_categories = []\n",
    "    for filename in glob.glob(data_path):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0].split('_')[0]\n",
    "        all_categories.append(category)\n",
    "        data = readFile(filename)\n",
    "        category_data[category] = data\n",
    "    \n",
    "    return category_data, all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original text is split into two parts, train and test, so that we can make sure that the model is not simply memorizing the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "['italian', 'czech', 'turkish', 'albanian', 'vietnamese', 'xhosa', 'danish', 'maori', 'swedish', 'lithuanian', 'english', 'spanish', 'portuguese', 'finnish', 'hungarian', 'romanian', 'french', 'norwegian', 'esperanto', 'german']\n"
     ]
    }
   ],
   "source": [
    "train_data_path = 'language_data/train/*_train.txt'\n",
    "test_data_path = 'language_data/test/*_test.txt'\n",
    "\n",
    "train_category_data, all_categories = get_category_data(train_data_path)\n",
    "test_category_data, test_all_categories = get_category_data(test_data_path)\n",
    "\n",
    "n_languages = len(all_categories)\n",
    "\n",
    "print(len(all_categories))\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1, dim=1)\n",
    "    category_i = top_i[:, 0]\n",
    "    return category_i\n",
    "\n",
    "# Turn string into long tensor\n",
    "def stringToTensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, 1).long().to(device)\n",
    "    input_text = []\n",
    "    for i in range(batch_size):\n",
    "        category = all_categories[random.randint(0, len(all_categories) - 1)]\n",
    "        line_start = random.randint(0, len(text[category])-chunk_len)\n",
    "        category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "        line = text[category][line_start:line_start+chunk_len]\n",
    "        input_text.append(line)\n",
    "        input_data[i] = stringToTensor(line)\n",
    "        target[i] = category_tensor\n",
    "    return input_data, target, input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Model\n",
    "====================\n",
    "\n",
    "For this classification task, we can use the same model we implement for the generation task which is located in `rnn/model.py`. See the `MP4_P2_generation.ipynb` notebook for more instructions. In this case each output vector of our RNN will have the dimension of the number of possible languages (i.e. `n_languages`). We will use this vector to predict a distribution over the languages.\n",
    "\n",
    "In the generation task, we used the output of the RNN at every time step to predict the next letter and our loss included the output from each of these predictions. However, in this task we use the output of the RNN at the end of the sequence to predict the language, so our loss function will use only the predicted output from the last time step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with chunk_len\n",
    "# chunk_len = 50\n",
    "chunk_len = 100\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "n_epochs = 2000\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "learning_rate = 0.01\n",
    "model_type = 'lstm'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rnn = RNN(n_letters, hidden_size, n_languages, model_type=model_type, n_layers=n_layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and the next hidden layer representation. The cross entropy loss should be computed over the last RNN output scores from the end of the sequence and the target classification tensor. Lastly, call backward on the loss and take an optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, target_tensor, data_tensor, optimizer, criterion, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - target_tensor: target character data tensor of shape (batch_size, 1)\n",
    "    - data_tensor: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    - batch_size: data batch size\n",
    "    \n",
    "    Returns:\n",
    "    - output: output from RNN from end of sequence \n",
    "    - loss: computed loss value as python float\n",
    "    TODO: Fill in the train function. You should initialize a hidden layer representation \n",
    "    using your RNN's init_hidden function, set the model gradients to zero, and loop over \n",
    "    each time step (character) in the input tensor. For each time step compute the output of \n",
    "    the of the RNN and the next hidden layer representation. The cross entropy loss should be \n",
    "    computed over the last RNN output scores from the end of \n",
    "    the sequence and the target classification tensor. Lastly, call backward on the loss and take an optimizer step.\n",
    "    \"\"\"\n",
    "    output, loss = None, None\n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    hidden = rnn.init_hidden(batch_size, device=device)\n",
    "    rnn.zero_grad()\n",
    "    for i in range(data_tensor.size()[1]):\n",
    "        output, hidden = rnn(data_tensor[:, i], hidden)\n",
    "        \n",
    "    loss = criterion(output, target_tensor.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    ##########       END      ##########\n",
    "\n",
    "    return output, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, data_tensor, seq_len=chunk_len, batch_size=BATCH_SIZE):\n",
    "    with torch.no_grad():\n",
    "        data_tensor = data_tensor.to(device)\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        for i in range(seq_len):\n",
    "            output, hidden = rnn(data_tensor[:,i], hidden)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def eval_test(rnn, category_tensor, data_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(rnn, data_tensor)\n",
    "        loss = criterion(output, category_tensor.squeeze())\n",
    "        return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 5% (0m 16s) 1.2087 1.0743 a iniquidade, e dao impio conselho nesta cidade; o / portuguese ✓\n",
      "Train accuracy: 0.3394\n",
      "100 10% (0m 29s) 0.8039 0.6702 le mit dans le lit, et mit a son chevet un tapis d / french ✓\n",
      "Train accuracy: 0.6528\n",
      "150 15% (0m 43s) 0.7057 0.5700 alabras, antes se fueron tras dioses ajenos para s / portuguese ✗ (spanish)\n",
      "Train accuracy: 0.7422\n",
      "200 20% (0m 57s) 0.6921 0.5675 tu Santo: A los que remitiereis los pecados, les s / french ✗ (spanish)\n",
      "Train accuracy: 0.7836\n",
      "250 25% (1m 11s) 0.6230 0.4731  stapinilor lor, si ochii roabei la mina stapinei  / italian ✗ (romanian)\n",
      "Train accuracy: 0.8098\n",
      "300 30% (1m 25s) 0.4879 0.3742 pp ett altare at HERREN, din Gud, overst pa denna  / swedish ✓\n",
      "Train accuracy: 0.8288\n",
      "350 35% (1m 39s) 0.4624 0.3477 d pa marken; og da han sa at folket kom ut av byen / norwegian ✓\n",
      "Train accuracy: 0.8426\n",
      "400 40% (1m 53s) 0.4355 0.3555 hitas; y fueron dadas por suerte a los hijos de Aa / spanish ✓\n",
      "Train accuracy: 0.8448\n",
      "450 45% (2m 7s) 0.3465 0.2701 os ni enterrados: seran por muladar sobre la haz d / portuguese ✗ (spanish)\n",
      "Train accuracy: 0.8652\n",
      "500 50% (2m 21s) 0.4059 0.3144 tt av kongens kostelige mat. Da lot kjellermestere / norwegian ✓\n",
      "Train accuracy: 0.8764\n",
      "550 55% (2m 35s) 0.2703 0.1765 s merveilles parmi tous les peuples. Car l'Eternel / french ✓\n",
      "Train accuracy: 0.8824\n",
      "600 60% (2m 49s) 0.4159 0.2967 un vi ankaux Judujo? Sed Amacja ne obeis; cxar tio / esperanto ✓\n",
      "Train accuracy: 0.8888\n"
     ]
    }
   ],
   "source": [
    "n_iters = 2000 #1000 #100000\n",
    "print_every = 50\n",
    "plot_every = 50\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "current_test_loss = 0\n",
    "all_losses = []\n",
    "all_test_losses = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "number_correct = 0\n",
    "for iter in range(1, n_iters + 1):\n",
    "    input_data, target_category, text_data = load_random_batch(train_category_data, chunk_len, BATCH_SIZE)\n",
    "    output, loss = train(rnn, target_category, input_data, optimizer, criterion)\n",
    "    current_loss += loss\n",
    "    \n",
    "    _, test_loss = eval_test(rnn, target_category, input_data)\n",
    "    current_test_loss += test_loss\n",
    "    \n",
    "    guess_i = categoryFromOutput(output)\n",
    "    number_correct += (target_category.squeeze()==guess_i.squeeze()).long().sum()\n",
    "    \n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        sample_idx = 0\n",
    "        guess = all_categories[guess_i[sample_idx]]\n",
    "        \n",
    "        category = all_categories[int(target_category[sample_idx])]\n",
    "        \n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %.4f %s / %s %s' % (iter, iter / n_iters * 100, time_since(start), loss, test_loss, text_data[sample_idx], guess, correct))\n",
    "        print('Train accuracy: {}'.format(float(number_correct)/float(print_every*BATCH_SIZE)))\n",
    "        number_correct = 0\n",
    "    \n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "        all_test_losses.append(current_test_loss / plot_every)\n",
    "        current_test_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss functions\n",
    "--------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses, color='b')\n",
    "plt.plot(all_test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate results\n",
    "-------------------\n",
    "\n",
    "We now vizualize the performance of our model by creating a confusion matrix. The ground truth languages of samples are represented by rows in the matrix while the predicted languages are represented by columns.\n",
    "\n",
    "In this evaluation we consider sequences of variable sizes rather than the fixed length sequences we used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 1  # needs to be set to 1 for evaluating different sequence lengths\n",
    "\n",
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_languages, n_languages)\n",
    "n_confusion = 1000\n",
    "num_correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(n_confusion):\n",
    "    eval_chunk_len = random.randint(10, 50) # in evaluation we will look at sequences of variable sizes\n",
    "    input_data, target_category, text_data = load_random_batch(test_category_data, chunk_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "    output = evaluate(rnn, input_data, seq_len=eval_chunk_len, batch_size=eval_batch_size)\n",
    "    \n",
    "    guess_i = categoryFromOutput(output)\n",
    "    category_i = [int(target_category[idx]) for idx in range(len(target_category))]\n",
    "    for j in range(eval_batch_size):\n",
    "        category = all_categories[category_i[j]] \n",
    "        confusion[category_i[j]][guess_i[j]] += 1\n",
    "        num_correct += int(guess_i[j]==category_i[j])\n",
    "        total += 1\n",
    "\n",
    "print('Test accuracy: ', float(num_correct)/float(n_confusion*eval_batch_size))\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_languages):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick out bright spots off the main axis that show which\n",
    "languages it guesses incorrectly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on User Input\n",
    "---------------------\n",
    "\n",
    "Now you can test your model on your own input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=5):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        input_data = stringToTensor(input_line).long().unsqueeze(0).to(device)\n",
    "        output = evaluate(rnn, input_data, seq_len=len(input_line), batch_size=1)\n",
    "\n",
    "    # Get top N categories\n",
    "    topv, topi = output.topk(n_predictions, dim=1)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        topv.shape\n",
    "        topi.shape\n",
    "        value = topv[0][i].item()\n",
    "        category_index = topi[0][i].item()\n",
    "        print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "        predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('This is a phrase to test the model on user input')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Kaggle submission file\n",
    "\n",
    "Once you have found a good set of hyperparameters submit the output of your model on the Kaggle test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT CHANGE KAGGLE SUBMISSION CODE ####\n",
    "import csv\n",
    "\n",
    "kaggle_test_file_path = 'language_data/kaggle_rnn_language_classification_test.txt'\n",
    "with open(kaggle_test_file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "output_rows = []\n",
    "for i, line in enumerate(lines):\n",
    "    sample = line.rstrip()\n",
    "    sample_chunk_len = len(sample)\n",
    "    input_data = stringToTensor(sample).unsqueeze(0)\n",
    "    output = evaluate(rnn, input_data, seq_len=sample_chunk_len, batch_size=1)\n",
    "    guess_i = categoryFromOutput(output)\n",
    "    output_rows.append((str(i+1), all_categories[guess_i]))\n",
    "\n",
    "submission_file_path = 'kaggle_rnn_submission.txt'\n",
    "with open(submission_file_path, 'w') as f:\n",
    "    output_rows = [('id', 'category')] + output_rows\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(output_rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
