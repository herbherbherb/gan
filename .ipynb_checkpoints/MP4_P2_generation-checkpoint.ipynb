{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# !pip install --ignore-installed unidecode\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 4573338\n",
      "train len:  4116004\n",
      "test len:  457334\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = './shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOUCESTER:\n",
      "Well, my good lord, I have inform'd them so.\n",
      "\n",
      "KING LEAR:\n",
      "Inform'd them! Dost thou understand me, man?\n",
      "\n",
      "GLOUCESTER:\n",
      "Ay, my good lord.\n",
      "\n",
      "KING LEAR:\n",
      "The king would speak with Cornwall; the dear\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "learning_rate = 0.01\n",
    "model_type = 'rnn'\n",
    "print_every = 100\n",
    "plot_every = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    batch_size = input.size(0)\n",
    "    chunk_len = input.size(1)\n",
    "    hidden = rnn.init_hidden(batch_size)\n",
    "    rnn.zero_grad()\n",
    "    for cur_chunk in range(chunk_len):\n",
    "        output, hidden = rnn(input[:, cur_chunk], hidden)\n",
    "        loss += criterion(output.view(batch_size, -1), target[:, cur_chunk])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return (loss.data[0]/chunk_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 4000 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1m 29s (100 2%) train loss: 1.9373, test_loss: 1.9664]\n",
      "When noce, thou comer:\n",
      "You lows? be your of the lallen hall and hon that and thear! in the jashald it  \n",
      "\n",
      "[2m 59s (200 5%) train loss: 1.8268, test_loss: 1.8558]\n",
      "Which do no reisn my wange.\n",
      "\n",
      "BERO:\n",
      "I commation thee of hosk shall trom overy would suming the father i \n",
      "\n",
      "[4m 29s (300 7%) train loss: 1.7315, test_loss: 1.8206]\n",
      "Why, for hander bape.\n",
      "\n",
      "SHALLOH:\n",
      "Then we heart all did dook beftices for my hear newnce of give,\n",
      "As Lor \n",
      "\n",
      "[5m 59s (400 10%) train loss: 1.7237, test_loss: 1.8182]\n",
      "Where with on with, is our mastles\n",
      "Hath judions'd of think upon mestable to his to this by this shall  \n",
      "\n",
      "[7m 29s (500 12%) train loss: 1.6937, test_loss: 1.7671]\n",
      "While, we pleasors,\n",
      "Fell him sleeps the Lord,\n",
      "Nor spore, make me.\n",
      "\n",
      "BORTEM:\n",
      "My good the pard.\n",
      "\n",
      "KING OF  \n",
      "\n",
      "[8m 58s (600 15%) train loss: 1.6842, test_loss: 1.7693]\n",
      "Who a fail, of her his can enceing. No! Are and rendies it jeishes, Rome with'd to be is I will theref \n",
      "\n",
      "[10m 28s (700 17%) train loss: 1.6691, test_loss: 1.7437]\n",
      "Whered have and which now I concear so can the man sumil, her ends; and he all me,\n",
      "Thou knave; what th \n",
      "\n",
      "[11m 58s (800 20%) train loss: 1.6765, test_loss: 1.7314]\n",
      "What some inst this a bolest bed, by the will will be have will us: but that what.\n",
      "\n",
      "DUCHESS OF YORDINI \n",
      "\n",
      "[13m 29s (900 22%) train loss: 1.6295, test_loss: 1.7425]\n",
      "Which the seek deart;\n",
      "And mase a trustart with the can well--\n",
      "come:\n",
      "For this we in think hear-bought n \n",
      "\n",
      "[14m 58s (1000 25%) train loss: 1.6546, test_loss: 1.7362]\n",
      "What nobs that in me of threath, I will all is them,\n",
      "Is freet how speak ingold sengan sworly to enemy\n",
      " \n",
      "\n",
      "[16m 29s (1100 27%) train loss: 1.6620, test_loss: 1.7341]\n",
      "Where never round of of me: what the little,\n",
      "Be with state, as not and be powers may as thy band! then \n",
      "\n",
      "[17m 58s (1200 30%) train loss: 1.6270, test_loss: 1.7488]\n",
      "Where imported,\n",
      "And me from him so be not my fellowing in thee call of his hangon, think not come ungo \n",
      "\n",
      "[19m 28s (1300 32%) train loss: 1.6410, test_loss: 1.7065]\n",
      "Where a best beligan! I could capt; and the parden all doth never with calker, sir,\n",
      "Which than on the  \n",
      "\n",
      "[20m 56s (1400 35%) train loss: 1.6251, test_loss: 1.7377]\n",
      "Whome his purpess rund again\n",
      "How he should I imprink Lords\n",
      "With heaven for answer;\n",
      "And be noble a gate \n",
      "\n",
      "[22m 26s (1500 37%) train loss: 1.6217, test_loss: 1.7068]\n",
      "Wheren sier this runger, God on the father:\n",
      "Sirlted he will to be this doth for the more beature her t \n",
      "\n",
      "[23m 56s (1600 40%) train loss: 1.6310, test_loss: 1.7345]\n",
      "Where and bring all\n",
      "Villief learner, for that burtain his wook, fain, ewand'd what thou lease and rest \n",
      "\n",
      "[25m 25s (1700 42%) train loss: 1.6519, test_loss: 1.7508]\n",
      "Whey he been for thee fegal.\n",
      "Go sprove the tongue come his timent your and must hath-busder on the los \n",
      "\n",
      "[26m 54s (1800 45%) train loss: 1.6268, test_loss: 1.7397]\n",
      "Whour head my cannot man, a never best and counter faillse with and Caerable, and be found them and po \n",
      "\n",
      "[28m 24s (1900 47%) train loss: 1.6106, test_loss: 1.7346]\n",
      "Whour mesce,\n",
      "Androw,\n",
      "Whel not that shrilt not poor gare on my send let our fait my lord his bear to yo \n",
      "\n",
      "[29m 52s (2000 50%) train loss: 1.6138, test_loss: 1.7564]\n",
      "Where which it eyes Hermaling--\n",
      "\n",
      "SILVIUS:\n",
      "Good grace, what?\n",
      "Which despain of were you shall not gost\n",
      "M \n",
      "\n",
      "[31m 21s (2100 52%) train loss: 1.6563, test_loss: 1.6960]\n",
      "When I will ast, Daught blood child them, let it in the fore, if speak a heart so their with to prease \n",
      "\n",
      "[32m 50s (2200 55%) train loss: 1.6026, test_loss: 1.7292]\n",
      "Where not blute, to and time a most offencied mine worth.\n",
      "\n",
      "PANDARUS:\n",
      "How curs friend are the prit the  \n",
      "\n",
      "[34m 19s (2300 57%) train loss: 1.6272, test_loss: 1.7052]\n",
      "Wheered in all to-depiritict to this preaus.\n",
      "\n",
      "QUEEN AESH:\n",
      "What hath with her sone to action great in t \n",
      "\n",
      "[35m 48s (2400 60%) train loss: 1.6131, test_loss: 1.7000]\n",
      "Where his thanks up with our sign\n",
      "That's new-lied it of still like no loud the give I mark, I fiest an \n",
      "\n",
      "[37m 17s (2500 62%) train loss: 1.5962, test_loss: 1.7209]\n",
      "Where are his crown me of remage of for, they or of the way the bare\n",
      "for mine do my sair be too any at \n",
      "\n",
      "[38m 46s (2600 65%) train loss: 1.6017, test_loss: 1.7085]\n",
      "Wheak assulente\n",
      "Of our come thee for shall wrelire may hath know a very from it of this pards is, if h \n",
      "\n",
      "[40m 15s (2700 67%) train loss: 1.5984, test_loss: 1.7062]\n",
      "Wheeds, and are the or that diet of from that yet give the but complial deward!\n",
      "\n",
      "EDGAR:\n",
      "Go once of the \n",
      "\n",
      "[41m 44s (2800 70%) train loss: 1.6281, test_loss: 1.7161]\n",
      "Whou will my shopes him come,\n",
      "My lord, and Roact,\n",
      "Altile, but the descourest and him.\n",
      "\n",
      "GLOUCESTER:\n",
      "And \n",
      "\n",
      "[43m 14s (2900 72%) train loss: 1.5826, test_loss: 1.7040]\n",
      "Where to fightry well. Beseech\n",
      "And which any our remblen and thee opreing on Antony and serve them com \n",
      "\n",
      "[44m 43s (3000 75%) train loss: 1.6235, test_loss: 1.7406]\n",
      "Whey had his ancestruse as mador:\n",
      "But I'll some in the very with us smorn, Ford in the still mother of \n",
      "\n",
      "[46m 12s (3100 77%) train loss: 1.5905, test_loss: 1.7155]\n",
      "Whet was.\n",
      "\n",
      "LORD BIANCA:\n",
      "Can me, prayer! 'Tis ear your belicome to this thave of his arm and should not \n",
      "\n",
      "[47m 40s (3200 80%) train loss: 1.5900, test_loss: 1.6739]\n",
      "Where scoglast: I have with a best, what should sta and your traithore in the into you, for thy reiess \n",
      "\n",
      "[49m 9s (3300 82%) train loss: 1.6398, test_loss: 1.6735]\n",
      "Whey serving what the will the slown;\n",
      "A have grace,\n",
      "Trong of master:--this for a more in the sirans, a \n",
      "\n",
      "[50m 38s (3400 85%) train loss: 1.6023, test_loss: 1.6984]\n",
      "Whou lain'd that sir,\n",
      "The woman whose holds and hate her and basuirge, her have in a place.\n",
      "\n",
      "Second Sh \n",
      "\n",
      "[52m 7s (3500 87%) train loss: 1.5835, test_loss: 1.7040]\n",
      "Whim AnLe death where I will man, this stunGered me,\n",
      "This mowarment I could with condord.\n",
      "\n",
      "PETRUCHIO:\n",
      " \n",
      "\n",
      "[53m 36s (3600 90%) train loss: 1.6155, test_loss: 1.7370]\n",
      "Where unseef is we him did did are a contend?\n",
      "\n",
      "Third he mean sualtinent of her many the bid to the lov \n",
      "\n",
      "[55m 4s (3700 92%) train loss: 1.5758, test_loss: 1.7319]\n",
      "Where and should not think,\n",
      "That me boss yates well and sees.\n",
      "\n",
      "PAROLLES:\n",
      "Whose thee leave to your cher \n",
      "\n",
      "[56m 33s (3800 95%) train loss: 1.5877, test_loss: 1.6945]\n",
      "Where he crown\n",
      "That the was from you puppence.\n",
      "\n",
      "LORD LYREN:\n",
      "Pear my man here addace and store of the c \n",
      "\n",
      "[58m 2s (3900 97%) train loss: 1.6257, test_loss: 1.7059]\n",
      "Where is steal, I will see heaven, but to every this, go him, if you lift of Prildon of put of the loo \n",
      "\n",
      "[59m 31s (4000 100%) train loss: 1.6113, test_loss: 1.6997]\n",
      "Whim, mumbert.\n",
      "\n",
      "PAGABLA:\n",
      "No\n",
      "Anderielf\n",
      "come.\n",
      "Uporning of the love your are not coat of lady, and outhou \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "# Adding scheduler for learning rate descent\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(rnn_optimizer, step_size=500, gamma=0.77)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    scheduler.step()\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network\n",
    "torch.save(rnn.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8ffb397390>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "print(len(all_losses))\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The--\n",
      "\n",
      "FALSTAFF:\n",
      "Repirimon, look as a mas me, comperved his days mine and to subdectle all thou have nothing, wonce, I wear of the trade a reportify the would not is with thy forestentle passes.\n",
      "\n",
      "MARIS:\n",
      "O honeat set this hole:\n",
      "A word, which the itsey, the sight I shame cortress in passous his she would conscience, no lord to revenge in my heart here and least the there:\n",
      "And it is the\n",
      "proud here sickly all enden your vile had for pain;\n",
      "The men\n",
      "Even to relays the bander'd a best them fixed of made to hour heard accuse as as no son, it to so peace but me now, to a hund, there is me. I the strept meire them and worth, and affections in it duke\n",
      "And you, not\n",
      "\n",
      "QUINCE:\n",
      "No, raps to ged fishing you, and that you think he is our bend not\n",
      "to them ducketly pity my stord, as I mean, I next may their wisence; an all, what thy intently!\n",
      "\n",
      "PRINCE HENRY:\n",
      "And when the charlour in vis and fawish his will he may well must it: falion, o'er.\n",
      "\n",
      "BENEDICK:\n",
      "He am honest\n",
      "Not dream;\n",
      "And best of promondain here constro\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
