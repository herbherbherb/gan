{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /home/jkane021/.local/lib/python2.7/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 4573338\n",
      "train len:  4116004\n",
      "test len:  457334\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = './shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "his dear blood doth owe?\n",
      "\n",
      "MONTAGUE:\n",
      "Not Romeo, prince, he was Mercutio's friend;\n",
      "His fault concludes but what the law should end,\n",
      "The life of Tybalt.\n",
      "\n",
      "PRINCE:\n",
      "And for that offence\n",
      "Immediately we do exi\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "learning_rate = 0.01\n",
    "model_type = 'rnn'\n",
    "print_every = 50\n",
    "plot_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    batch_size = input.size(0)\n",
    "    chunk_len = input.size(1)\n",
    "    hidden = rnn.init_hidden(batch_size)\n",
    "    rnn.zero_grad()\n",
    "    for cur_chunk in range(chunk_len):\n",
    "        output, hidden = rnn(input[:, cur_chunk], hidden)\n",
    "        loss += criterion(output.view(batch_size, -1), target[:, cur_chunk])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return (loss.data[0]/chunk_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 47s (50 1%) train loss: 2.0825, test_loss: 2.0981]\n",
      "Whe and\n",
      "Wime to kooss not hour maje you by mur swears man, theng mave a fot on I he sere and for the t \n",
      "\n",
      "[1m 30s (100 2%) train loss: 1.9415, test_loss: 1.9533]\n",
      "Whis shall molellod our you son he lean the my ca here, paster vill, ance they a I ur so have it to sh \n",
      "\n",
      "[2m 15s (150 3%) train loss: 1.8593, test_loss: 1.9096]\n",
      "What hears seart sco componguurs for good of the incher.\n",
      "\n",
      "BERCESS:\n",
      "Whold be the preband plark on this  \n",
      "\n",
      "[2m 59s (200 4%) train loss: 1.8134, test_loss: 1.8573]\n",
      "Whis lord.\n",
      "\n",
      "ACHINCE:\n",
      "I crivers of shall bust.\n",
      "\n",
      "MARLINA:\n",
      "For you sage\n",
      "Whose it, state and where his man \n",
      "\n",
      "[3m 43s (250 5%) train loss: 1.7594, test_loss: 1.8624]\n",
      "Whit hate thee and be grawhock, be my sie.\n",
      "\n",
      "DUCHIIAN PAURINANO:\n",
      "But stenrand\n",
      "Ind here the deason it.\n",
      "\n",
      " \n",
      "\n",
      "[4m 26s (300 6%) train loss: 1.7685, test_loss: 1.8049]\n",
      "Why, come.\n",
      "\n",
      "APRINCENDER:\n",
      "Ruse beass me man here of would storess sir, my father's ment to that but is  \n",
      "\n",
      "[5m 10s (350 7%) train loss: 1.7633, test_loss: 1.7491]\n",
      "Why,\n",
      "Do the way. Citurniage of short:\n",
      "To the dened should not strongs nefold be think\n",
      "Thou larum.\n",
      "\n",
      "KEN \n",
      "\n",
      "[5m 54s (400 8%) train loss: 1.7081, test_loss: 1.7918]\n",
      "Why, that aments thou so! then she have arge is infort as do, Anthan desefored the yent from the great \n",
      "\n",
      "[6m 38s (450 9%) train loss: 1.7351, test_loss: 1.7661]\n",
      "Why, seet begum the such edeing done,\n",
      "The good with the procking to what all read his pleasones, why,  \n",
      "\n",
      "[7m 22s (500 10%) train loss: 1.7058, test_loss: 1.7591]\n",
      "Where in them not hame the is\n",
      "Think of is the chiment exprarge.\n",
      "\n",
      "PORD:\n",
      "Me sake the kided decetice deli \n",
      "\n",
      "[8m 7s (550 11%) train loss: 1.7104, test_loss: 1.7519]\n",
      "Why for you are you this prids Valaged with at a secrot, whicher is trid batter; I am you think from t \n",
      "\n",
      "[8m 50s (600 12%) train loss: 1.6495, test_loss: 1.7637]\n",
      "When to truthone, by change the should the to truther--\n",
      "\n",
      "POLENWARD IV:\n",
      "Ers is perssen\n",
      "It despring sue? \n",
      "\n",
      "[9m 34s (650 13%) train loss: 1.7001, test_loss: 1.7452]\n",
      "When shall my voine on cape.\n",
      "\n",
      "SIMONIND:\n",
      "The did sweel; I most, freat mine nature: is ancas your makst  \n",
      "\n",
      "[10m 18s (700 14%) train loss: 1.6824, test_loss: 1.7389]\n",
      "Why, the are beggar my resef my loves and meandood to we pather:\n",
      "And honour him for the will oftes;\n",
      "Wh \n",
      "\n",
      "[11m 2s (750 15%) train loss: 1.6988, test_loss: 1.7732]\n",
      "Wher let father?\n",
      "\n",
      "DEMIANA:\n",
      "Be your take and are being with for is hearth,\n",
      "Thus call of some, ancounter \n",
      "\n",
      "[11m 46s (800 16%) train loss: 1.6463, test_loss: 1.7335]\n",
      "What I know lands of somed this pretty we are to not the Marge she way his pretch me a she not to all  \n",
      "\n",
      "[12m 30s (850 17%) train loss: 1.6643, test_loss: 1.7327]\n",
      "Where ne call our steading hap upon.\n",
      "\n",
      "ORINA:\n",
      "Fired is my griens your think the fair show hand me to ma \n",
      "\n",
      "[13m 15s (900 18%) train loss: 1.6726, test_loss: 1.7208]\n",
      "What such acking speak a prove amer of will's to rever\n",
      "Do, and him.\n",
      "\n",
      "LUCIUS:\n",
      "Beniash--\n",
      "O? Have mistran \n",
      "\n",
      "[13m 59s (950 19%) train loss: 1.6612, test_loss: 1.7561]\n",
      "Wh Shall\n",
      "vower you; what my some more with, courth,\n",
      "Whiles the time.\n",
      "\n",
      "KING HENRY IV:\n",
      "I commonder much  \n",
      "\n",
      "[14m 43s (1000 20%) train loss: 1.7327, test_loss: 1.7462]\n",
      "Wh Since will not to at you'll are weaked to her lasts?\n",
      "\n",
      "FALSTAFF:\n",
      "By hum in the partice partion now c \n",
      "\n",
      "[15m 27s (1050 21%) train loss: 1.6650, test_loss: 1.7434]\n",
      "When make the matter lady to the gipess to sir, and this queen the late of prew the short a lither low \n",
      "\n",
      "[16m 11s (1100 22%) train loss: 1.6525, test_loss: 1.7648]\n",
      "Where thinks him though now ye rewitwerus rewich, and the life to the dead to the one my long the doth \n",
      "\n",
      "[16m 55s (1150 23%) train loss: 1.6345, test_loss: 1.7235]\n",
      "Who, and a speak it to she, farcess\n",
      "Or desire\n",
      "Desuik the know wilt with this love:\n",
      "Neek others swink w \n",
      "\n",
      "[17m 39s (1200 24%) train loss: 1.6906, test_loss: 1.7548]\n",
      "Where a faticound's a good at the shall creir a langs they so a some\n",
      "Lord:\n",
      "His man his againswed his c \n",
      "\n",
      "[18m 23s (1250 25%) train loss: 1.6553, test_loss: 1.7451]\n",
      "Who conhers and to thee: sir.\n",
      "\n",
      "MISTRESSA:\n",
      "How I would it I get that son his is it it with the more, wh \n",
      "\n",
      "[19m 7s (1300 26%) train loss: 1.6599, test_loss: 1.7253]\n",
      "Who shall from you give a sweet lord of sweedius marry, leciaus; you will will chass and know you, you \n",
      "\n",
      "[19m 51s (1350 27%) train loss: 1.6451, test_loss: 1.7232]\n",
      "Why lord we have in your cate of this are an young his I send and deach whose make in your wall stand  \n",
      "\n",
      "[20m 35s (1400 28%) train loss: 1.6639, test_loss: 1.7493]\n",
      "Who Death, I play to than them wrong\n",
      "That you privis dead of it, as adon the bro'sty plain! I thing; t \n",
      "\n",
      "[21m 19s (1450 28%) train loss: 1.5976, test_loss: 1.7219]\n",
      "When.\n",
      "\n",
      "AGAND:\n",
      "They hell, prison.\n",
      "\n",
      "GLOUCESTER:\n",
      "At down I will prace,\n",
      "I am a men you are mornio, some so \n",
      "\n",
      "[22m 3s (1500 30%) train loss: 1.6320, test_loss: 1.7312]\n",
      "Wh frignes this my love, percy stand to that he that my be stand, copen him might to now cormone are y \n",
      "\n",
      "[22m 47s (1550 31%) train loss: 1.6529, test_loss: 1.7414]\n",
      "What not Griffest in the honourable Soldor blood,\n",
      "Eron, doth good hearts, I pretence unghing a more gr \n",
      "\n",
      "[23m 32s (1600 32%) train loss: 1.6755, test_loss: 1.6935]\n",
      "Which and keep against their.\n",
      "\n",
      "First harge,\n",
      "And come ir now?\n",
      "\n",
      "Serving grace, set at full I eatt her th \n",
      "\n",
      "[24m 16s (1650 33%) train loss: 1.6601, test_loss: 1.7132]\n",
      "When me us, in my dising of this the sain?\n",
      "\n",
      "TRIA:\n",
      "How love: he sont her brother with in, and the hast  \n",
      "\n",
      "[25m 0s (1700 34%) train loss: 1.6532, test_loss: 1.7260]\n",
      "Whicher in my forthelstand in the she hate a met of mine empolour speak in the thinking to my all him. \n",
      "\n",
      "[25m 44s (1750 35%) train loss: 1.6508, test_loss: 1.7055]\n",
      "Why there of the read me.\n",
      "\n",
      "ANTIPHOLUS OF KING HENRY\n",
      "SOLUR:\n",
      "Hese:\n",
      "I thine how he manom, bear is fauttly \n",
      "\n",
      "[26m 28s (1800 36%) train loss: 1.6469, test_loss: 1.7170]\n",
      "Who have hangs, and not behing my fair to curse to this that spruch this is make than is let thee ther \n",
      "\n",
      "[27m 12s (1850 37%) train loss: 1.6395, test_loss: 1.7524]\n",
      "Who engerouse are and love of it slain,\n",
      "And cristanceed\n",
      "donain his eep for and thank thy king hather n \n",
      "\n",
      "[27m 56s (1900 38%) train loss: 1.6767, test_loss: 1.7446]\n",
      "Who catter you and the mountion Cant love his shall I sir,\n",
      "Do you do,\n",
      "Sirn stoneys and the satchiniaid \n",
      "\n",
      "[28m 40s (1950 39%) train loss: 1.6421, test_loss: 1.7310]\n",
      "Where on, compames and the thus down sicklian; and waram: and honourable;\n",
      "And, princes, I be cames hei \n",
      "\n",
      "[29m 24s (2000 40%) train loss: 1.6592, test_loss: 1.7193]\n",
      "What elownd, my lord, and in a most now, thou lies murded him cried. What do be from him goo; be befor \n",
      "\n",
      "[30m 8s (2050 41%) train loss: 1.6314, test_loss: 1.7275]\n",
      "Why repen. He warry\n",
      "Spoke let Troy's thim which the joy.\n",
      "\n",
      "PERTIO:\n",
      "Noble own of heart your the prail, a \n",
      "\n",
      "[30m 52s (2100 42%) train loss: 1.6399, test_loss: 1.7411]\n",
      "What part rook of your and fortunes for our as the persond ye from on my trunk and came,\n",
      "Hast, stronge \n",
      "\n",
      "[31m 36s (2150 43%) train loss: 1.6216, test_loss: 1.7380]\n",
      "Why manged?\n",
      "\n",
      "MARITES:\n",
      "If I priced farewer\n",
      "Stid, in the fine thangn\n",
      "doisures to the could I what her co \n",
      "\n",
      "[32m 21s (2200 44%) train loss: 1.6465, test_loss: 1.7277]\n",
      "Who am not some is sucles.\n",
      "\n",
      "ROSALIND:\n",
      "No, and thee not.\n",
      "\n",
      "PRINCE HENRY:\n",
      "How any in the be I have some s \n",
      "\n",
      "[33m 5s (2250 45%) train loss: 1.6522, test_loss: 1.7501]\n",
      "Whath I would from thee. But you thus forment, thou those there am, when in thee, and a britourets of  \n",
      "\n",
      "[33m 49s (2300 46%) train loss: 1.6416, test_loss: 1.7387]\n",
      "Why feging soldier:\n",
      "E'll with our Edwarra-hold of the elding the vimlors troubles fly, I must so'stion \n",
      "\n",
      "[34m 32s (2350 47%) train loss: 1.6551, test_loss: 1.7357]\n",
      "Whather;\n",
      "And when will I dessing in no our for hand of make now,\n",
      "I be hather that shall tongentless of \n",
      "\n",
      "[35m 16s (2400 48%) train loss: 1.6313, test_loss: 1.7417]\n",
      "Whicher's struckous mistress the death,\n",
      "My lord; and as brafful as a dispossich him a any compatters f \n",
      "\n",
      "[36m 0s (2450 49%) train loss: 1.6238, test_loss: 1.7062]\n",
      "Whichers?\n",
      "\n",
      "DUDGNIA:\n",
      "It meed of the by thee thee, to men'd.\n",
      "\n",
      "REMIO:\n",
      "I prumengit.\n",
      "\n",
      "MARK ANTONY:\n",
      "I enesha \n",
      "\n",
      "[36m 44s (2500 50%) train loss: 1.6328, test_loss: 1.7089]\n",
      "Why kill not that that she my at her not to doth answerch to stands, to the wife, I be a yence her,\n",
      "Or \n",
      "\n",
      "[37m 28s (2550 51%) train loss: 1.6096, test_loss: 1.7559]\n",
      "Where alint place parts.\n",
      "\n",
      "STAMILLO:\n",
      "We stir as his tanged to he blind to fortus mottle your doise of t \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38m 12s (2600 52%) train loss: 1.6219, test_loss: 1.7169]\n",
      "Why, we may good welcome now, if he love, like roson is so confest one pleasure\n",
      "to not to your gullus: \n",
      "\n",
      "[38m 56s (2650 53%) train loss: 1.6212, test_loss: 1.7285]\n",
      "What find together.\n",
      "\n",
      "DOMITO:\n",
      "The way a stay, I did you seat love, and madam?\n",
      "\n",
      "COUNTESSA:\n",
      "I do merith e \n",
      "\n",
      "[39m 40s (2700 54%) train loss: 1.6581, test_loss: 1.7429]\n",
      "Wh:\n",
      "Il sent nating and then, that say!\n",
      "\n",
      "First your the will be so winte;\n",
      "Have this all your first but  \n",
      "\n",
      "[40m 24s (2750 55%) train loss: 1.6371, test_loss: 1.7198]\n",
      "Whom hid father? By to make there, in my call him with root as with speed times and alive the herrace  \n",
      "\n",
      "[41m 8s (2800 56%) train loss: 1.6585, test_loss: 1.7352]\n",
      "What since and his\n",
      "Though thee, I'll hambood:\n",
      "Sir, and make the speaked alours,\n",
      "For her leave\n",
      "That can \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network\n",
    "torch.save(rnn.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff4581d4cf8>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "print(len())\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This thees shephellow\n",
      "Taunine look you my soul in all of the grace you, speak?\n",
      "\n",
      "PROSPER:\n",
      "Lord office by with sa'tlest me. First be lords.\n",
      "\n",
      "CASSIUS:\n",
      "Good poor word.\n",
      "\n",
      "SALUNT:\n",
      "I have excempous for my save you.\n",
      "\n",
      "Secully.\n",
      "\n",
      "SIMON:\n",
      "Thou good;\n",
      "With two recius and say the heavion.\n",
      "\n",
      "MARCUS ANDRONIUS:\n",
      "And both supprect the of my live the all never did fellow, and benal and presever speines.\n",
      "\n",
      "WARWICK:\n",
      "And the king master fear me onces; I nove and good heart of all the die I tong:\n",
      "Thy day, afon my grown our nor one call the furgher hath fours:\n",
      "Mardred for begiet all your rection fool.\n",
      "\n",
      "ROSALINE:\n",
      "Faull the so nead; there well mine supposes,\n",
      "Upon her dispannew;\n",
      "With roye, sir, but he Lord me of like my little fair tell the true sweet impancer was stringer discover deforthing o'ers bid it.\n",
      "\n",
      "CASSIUS:\n",
      "You was the hight out the name revenge borner of my darie.\n",
      "\n",
      "MISTRESS OF OF YIO:\n",
      "What would seet\n",
      "And some, that them:\n",
      "I sworty like me.\n",
      "\n",
      "AGRILLES:\n",
      "Such standing not sorrow crubure belive no offends,\n",
      "O his adl\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
