{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /home/jkane021/.local/lib/python2.7/site-packages\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 4573338\n",
      "train len:  4116004\n",
      "test len:  457334\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = './shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yond a common joy, and set it down\n",
      "With gold on lasting pillars: In one voyage\n",
      "Did Claribel her husband find at Tunis,\n",
      "And Ferdinand, her brother, found a wife\n",
      "Where he himself was lost, Prospero his d\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "learning_rate = 0.01\n",
    "model_type = 'rnn'\n",
    "print_every = 50\n",
    "plot_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    batch_size = input.size(0)\n",
    "    chunk_len = input.size(1)\n",
    "    hidden = rnn.init_hidden(batch_size)\n",
    "    rnn.zero_grad()\n",
    "    for cur_chunk in range(chunk_len):\n",
    "        output, hidden = rnn(input[:, cur_chunk], hidden)\n",
    "        loss += criterion(output.view(batch_size, -1), target[:, cur_chunk])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return (loss.data[0]/chunk_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 41s (50 1%) train loss: 2.5127, test_loss: 2.4967]\n",
      "Whid se ithots t and mellougimayoke.\n",
      "Whelds inte the mis omyous athedotimaneler thid fingrsthangerit o \n",
      "\n",
      "[1m 23s (100 2%) train loss: 2.4823, test_loss: 2.4848]\n",
      "Whe e, the the t t pee on howins thouraitr h n at and ntome thothapar f eng achirillyou bees angakear  \n",
      "\n",
      "[2m 5s (150 3%) train loss: 2.4843, test_loss: 2.4902]\n",
      "Whimy fes, thiveace d the\n",
      "\n",
      "HOSInte pafe m ngsastat s!\n",
      "\n",
      "G tau hed I hatar t\n",
      " arapowad;\n",
      "K: f these st br \n",
      "\n",
      "[2m 47s (200 4%) train loss: 2.4992, test_loss: 2.4863]\n",
      "Whes mar withersthe thilvenond d barse y TO:\n",
      "JARDUCAGUFous forully athe meatheanevenoullr hthay.\n",
      "Thand \n",
      "\n",
      "[3m 29s (250 5%) train loss: 2.4839, test_loss: 2.5178]\n",
      "Whiereall ngey houe Whe diswis thiche tet wo lllin lo:\n",
      "\n",
      "Mecherr ak thinory cangasee toulethal tharacim \n",
      "\n",
      "[4m 10s (300 6%) train loss: 2.4794, test_loss: 2.5088]\n",
      "Whal.\n",
      "Whas y:\n",
      "g d achoustamathe ave hithin'the pand an.\n",
      "NAnilly the ea INENAs fele, nce:\n",
      "\n",
      "ICTE:\n",
      "\n",
      "\n",
      "The  \n",
      "\n",
      "[4m 53s (350 7%) train loss: 2.5036, test_loss: 2.4847]\n",
      "Whendiay ad to s mers ert be boudeded mble out'd a veatouthe LAn as iouileeersusead awowillietoul.\n",
      "\n",
      "Ye \n",
      "\n",
      "[5m 35s (400 8%) train loss: 2.4732, test_loss: 2.4597]\n",
      "Whes ar de asthipooun bame whegor asared bay,\n",
      "\n",
      "Whad\n",
      "VI'shouns atars wine, les ang vero w, lenamesse y  \n",
      "\n",
      "[6m 16s (450 9%) train loss: 2.4885, test_loss: 2.5043]\n",
      "Whonstharmy g thag sers ll ifo w werder ndo thianeritoramanongee thir ther the firthord athedus, morer \n",
      "\n",
      "[6m 58s (500 10%) train loss: 2.4868, test_loss: 2.4773]\n",
      "Wher ithatase thedse,\n",
      "I s\n",
      "LORY:\n",
      "The s ofr thesu g erasie y yotheat tar tof ck wintowidinthetaks ld tha \n",
      "\n",
      "[7m 40s (550 11%) train loss: 2.4792, test_loss: 2.5061]\n",
      "Whes ad bl be ld onor he womsito se, y be oura s as, d atin\n",
      "\n",
      "Th wit heas, ltou t I s t I hilbeave thod \n",
      "\n",
      "[8m 22s (600 12%) train loss: 2.4693, test_loss: 2.4867]\n",
      "Whe comes whachean'd d hal atot!\n",
      "ARor hime aver? w aset d\n",
      "ARO:\n",
      "\n",
      "\n",
      "ALYowe wis\n",
      "Thand torannde ke t winath \n",
      "\n",
      "[9m 4s (650 13%) train loss: 2.4627, test_loss: 2.4767]\n",
      "Whyad.\n",
      "HENon whes be ischeat, pef beathe t 'tishenerath I moure I foupt, whind s ine maver g than is i \n",
      "\n",
      "[9m 46s (700 14%) train loss: 2.4737, test_loss: 2.5212]\n",
      "Whirthaitots h de fowarerovedy od fan he\n",
      "ORere yolo y d t flot ld f oue mer thiesor ongit G te tise th \n",
      "\n",
      "[10m 28s (750 15%) train loss: 2.4810, test_loss: 2.4886]\n",
      "Whalcace athisould de s picthe t me pe wot.\n",
      "theber cos thit byond s my there ave VIsu st t e womer mar \n",
      "\n",
      "[11m 10s (800 16%) train loss: 2.4686, test_loss: 2.5000]\n",
      "Whind morathishild we wne my thed I y a d al thelds s e n won sse;\n",
      "\n",
      "I t illd the tharioulousbakisert a \n",
      "\n",
      "[11m 51s (850 17%) train loss: 2.4799, test_loss: 2.5101]\n",
      "Whiso hon r co this wal cour athanothe I I akn Jer.\n",
      "Yot aghape'd yo th antoveteforend t t I hes he hay \n",
      "\n",
      "[12m 33s (900 18%) train loss: 2.4806, test_loss: 2.4741]\n",
      "Whe ist ICHemononourgof Whis lt ke h, he ng wir the t.\n",
      "\n",
      "Mald bens t s t jos the tond mal inge thon y f \n",
      "\n",
      "[13m 15s (950 19%) train loss: 2.4925, test_loss: 2.5038]\n",
      "Who wis ou m hinrtochar h,\n",
      "Myowal te t CEThar ade sosucou y s, gre ym thad NI le ontanorise t ay g thi \n",
      "\n",
      "[13m 57s (1000 20%) train loss: 2.4857, test_loss: 2.4949]\n",
      "Wh r the whowou' artof Pay n. f heanedshe an wanchisthar medoshe shand icrere so munghe o hio gur' m g \n",
      "\n",
      "[14m 39s (1050 21%) train loss: 2.4647, test_loss: 2.4947]\n",
      "Whtint tige hima be alle: bye tis, mmelofe can io tongs w de w\n",
      "An n a m chimer tong s ws tim t af t si \n",
      "\n",
      "[15m 21s (1100 22%) train loss: 2.4909, test_loss: 2.5069]\n",
      "Whe and, vitouceand w, t's ithima win dove t necllsplivies gl st ou tld is.\n",
      "Hacalas t myelanorthof ARI \n",
      "\n",
      "[16m 3s (1150 23%) train loss: 2.4823, test_loss: 2.4824]\n",
      "Whin thonond shamunt d engoit tielourag, Fon?\n",
      "BEDUSenghansthere touris arthe the merererure h t, fal f \n",
      "\n",
      "[16m 45s (1200 24%) train loss: 2.4843, test_loss: 2.4811]\n",
      "Wh ar mees f es gsh se BRGor s ian cou be ce the athipar by Isirs ones w r thar se ttha im heresote he \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-f7697cd3fcd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training for %d epochs...\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mload_random_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-102c2054d5b7>\u001b[0m in \u001b[0;36mload_random_batch\u001b[0;34m(text, chunk_len, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mend_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchunk_len\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-cfb43361b0c5>\u001b[0m in \u001b[0;36mchar_tensor\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_characters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network\n",
    "torch.save(classifier.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2d2a2e1048>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAH+tJREFUeJzt3W2MXFed5/Hvv566q9rdXf1sd9ttJ8QRhAXHiTcwIlrCokEZFoYdabUCJGZeTdAINIkW7YhBO+xqJF6hzaKdYRZlBsTMCAatlABhCZAIhQADRNjGieOYJMZJ7PZj291V1d1V/VT13xf3Vnd1ubtdbrdd7bq/j3R1b9263X3q+vp3Tp0695S5OyIiEh2xZhdARERuLgW/iEjEKPhFRCJGwS8iEjEKfhGRiFHwi4hEjIJfRCRiFPwiIhGj4BcRiZhEswuwmv7+ft+zZ0+ziyEicss4dOjQJXcfaOTYLRn8e/bs4eDBg80uhojILcPM3mz0WHX1iIhEjIJfRCRiFPwiIhGj4BcRiRgFv4hIxCj4RUQiRsEvIhIxLRP87s7f/Pg1nnt1vNlFERHZ0lom+M2Mx356kp+8crHZRRER2dJaJvgBujNJ8sWFZhdDRGRLa6ngz2aSTBbnm10MEZEtraWCvyeTIldSi19EZD0tFfzdaXX1iIhcTUsFfzaTVItfROQqWiv40ylyxXkqFW92UUREtqzWCv5MkorD1Nxis4siIrJltVjwpwDUzy8iso7WCv50EkBDOkVE1tFSwd/TEQS/PuAVEVnbVYPfzHaZ2bNm9rKZHTOzh1c55r+a2ZFwecnMymbWGz73oJm9YmYnzOyzN+JFVHWng66enFr8IiJraqTFvwh8xt3vAt4NfMrM7qo9wN2/6O53u/vdwF8Cz7n7hJnFgS8DfwDcBXys/mc3UzYTtPjzavGLiKzpqsHv7ufc/XC4PQUcB0bW+ZGPAf8Sbt8HnHD3k+4+D3wL+Mj1FXlt3dU+/hkFv4jIWq6pj9/M9gD7gefXeD4DPAg8Hu4aAU7XHDLG+pXGdUnGY3S2JciV1NUjIrKWhoPfzLYRBPoj7l5Y47APA//q7hPXWhAze8jMDprZwfHxjc+prxk6RUTW11Dwm1mSIPS/4e5PrHPoR1nu5gE4A+yqebwz3HcFd3/M3Q+4+4GBgYFGirUqzdApIrK+Rkb1GPBV4Li7P7rOcd3Ae4Hv1uz+NbDXzG4zsxRBxfDk9RV5fZqhU0RkfYkGjnkP8AngqJkdCfd9DhgFcPevhPv+CHja3WeqP+jui2b2aeBHQBz4mrsf26zCr6Y7neTMZOlG/gkRkVvaVYPf3X8OWAPHfR34+ir7nwKe2kDZNkRdPSIi62upO3ch6OrJlxY0Q6eIyBpaLvi705qhU0RkPS0X/JqhU0Rkfa0X/JqhU0RkXS0X/JqhU0RkfS0X/JqhU0RkfS0X/NUZOnPq4xcRWVXrBX9awS8isp6WC/6EZugUEVlXywU/aIZOEZH1tGTwa9oGEZG1tWTwa4ZOEZG1NTI7563BHR59FPbvpzud1QydIiJraJ0Wvxn89V/Dk0+qq0dEZB2tE/wAg4Nw4YJm6BQRWUdrBf/QEFy4oBk6RUTW0ZLBrxk6RUTW1prBrxk6RUTW1HrBf/kyPW3BN0VqSKeIyJVaK/gHBwHoLRYAzdApIrKa1gr+oSEAslMTgCZqExFZTUsGf1fuMqDgFxFZTUsGf/zSuGboFBFZQ0sGPxcu0J1JqsUvIrKK1gr+bdsgnYaLF4OJ2vThrojIFVor+M2Wpm3IZpIazikisorWCn5YMW2D7twVEblSywa/ZugUEVldywa/ZugUEVldawb/+DjdbXHN0CkisorWC/7BQSiXGVwsApq2QUSkXusFfziWf3BmEtDduyIi9Vo2+Hurwa8hnSIiK1w1+M1sl5k9a2Yvm9kxM3t4jeMeMLMj4THP1ex/w8yOhs8d3MzCr6o6X0++2uJXV4+ISK1EA8csAp9x98Nm1gkcMrNn3P3l6gFmlgX+DnjQ3U+Z2WDd73ifu1/avGKvIwz+bYUJYFBdPSIida7a4nf3c+5+ONyeAo4DI3WHfRx4wt1Phcdd3OyCNiybhUSCzERQzyj4RURWuqY+fjPbA+wHnq976k6gx8x+YmaHzOyPa55z4Olw/0PXU9iGxGIwOEhs/KJm6BQRWUUjXT0AmNk24HHgEXcvrPJ77gXeD6SBX5rZr9z9VeB+dz8Tdv88Y2a/dfefrvL7HwIeAhgdHd3Yq6mqTttwp2boFBGp11CL38ySBKH/DXd/YpVDxoAfuftM2Jf/U2AfgLufCdcXgW8D9632N9z9MXc/4O4HBgYGrv2V1Kq5e1cf7oqIrNTIqB4Dvgocd/dH1zjsu8D9ZpYwswzwLuC4mXWEHwhjZh3AB4CXNqfo66iZr0fDOUVEVmqkq+c9wCeAo2Z2JNz3OWAUwN2/4u7HzeyHwItABfgHd3/JzG4Hvh3UHSSAb7r7Dzf7RVxhcBAuXqS7PcGZydIN/3MiIreSqwa/u/8csAaO+yLwxbp9Jwm7fG6qoSGYm2OIOf5VXT0iIiu03p27sDSWf2SuoBk6RUTqtHTwDxTzmqFTRKROSwd//0wO0LQNIiK1Wjr4ezRDp4jIFVoz+Pv6wIyu/ASgGTpFRGq1ZvAnEtDfT0fuMqCuHhGRWq0Z/ABDQ7RrojYRkSu0dPCnLo0DCn4RkVotHfymGTpFRK7QusE/OBjM0JnRDJ0iIrVaN/iHhmB6mu2Jij7cFRGp0drBD4wuTGk4p4hIjZYP/uG5grp6RERqNPwNXLecMPh3zObJmbp6RESqWjf4BwcBGCjlyRPM0BmLXXV2aRGRlte6XT1h8PdOT2qGThGRGq0b/G1tkM2SnapO1KbuHhERaOXgBxgaojNfna9HH/CKiEAEgj8zGQa/hnSKiAARCP62yepEberqERGBVg/+wUESmqhNRGSF1g7+oSFik5MkywsKfhGRUMsHP8DuxWnN0CkiEopO8KvFLyICRCT4dy1M6cNdEZFQawd/ePfu8HxBwzlFREKtHfxhi3+wlFdXj4hIqLWDv6MDOjoYmMmpq0dEJNTawQ8wNETv9CT5UjBDp4hI1EUi+LsKE5qhU0Qk1PrBPzhIR34C0LQNIiIQheAfGiKzNF+PPuAVEYlE8CcnJ4hVyhrSKSJCA8FvZrvM7Fkze9nMjpnZw2sc94CZHQmPea5m/4Nm9oqZnTCzz25m4RsyNIRVKvSWCurqERGhse/cXQQ+4+6HzawTOGRmz7j7y9UDzCwL/B3woLufMrPBcH8c+DLw+8AY8Gsze7L2Z2+4cCx//0xOXT0iIjTQ4nf3c+5+ONyeAo4DI3WHfRx4wt1PhcddDPffB5xw95PuPg98C/jIZhW+IeHdu31F3cQlIgLX2MdvZnuA/cDzdU/dCfSY2U/M7JCZ/XG4fwQ4XXPcGFdWGtXf/ZCZHTSzg+Pj49dSrPVV5+uZL2iGThERGuvqAcDMtgGPA4+4e2GV33Mv8H4gDfzSzH51LQVx98eAxwAOHDiweXdahcE/Ml/gpFr8IiKNBb+ZJQlC/xvu/sQqh4wBl919Bpgxs58C+8L9u2qO2wmcub4iX6Pubkil2DFb4LA+3BURaWhUjwFfBY67+6NrHPZd4H4zS5hZBngXwWcBvwb2mtltZpYCPgo8uTlFb5AZDA0xVMppOKeICI21+N8DfAI4amZHwn2fA0YB3P0r7n7czH4IvAhUgH9w95cAzOzTwI+AOPA1dz+2ya/h6oaG6J3WqB4REWgg+N3954A1cNwXgS+usv8p4KkNlW6zDA6S/e3rGscvIkIU7tyFpYnaNEOniEiEgj+Tm6BScaZmNUOniERbZII/vrhA9+y0xvKLSORFJvhB0zaIiEBUgj+ctqG/qCGdIiLRCP4VLX519YhItEUr+Ivq6hERiUbw9/Xh8bj6+EVEiErwx2LYwAA7ZvNMqqtHRCIuGsEPMDjI0FyBvD7cFZGIi07wDw0xoA93RUSiFfx9M5MazikikRep4O8uTJCbUYtfRKItUsGfmp9jPpdvdklERJoqUsEPkLo8rhk6RSTSohP84bQNPdN5zdApIpEWneAPW/wDxUnN0CkikRa54NfduyISddEJ/oEBIAx+DekUkQiLTvAnk5R7e8OJ2tTVIyLRFZ3gB3xgkD519YhIxEUq+GPbt2tqZhGJvIgF/xCDRc3QKSLRFqngZ2iI/pmcZugUkUiLXPBvm5thOj/V7JKIiDRNtII/vHvXL1xsckFERJonWsEf3sSVuDTe5IKIiDRPJIM/dUktfhGJrkgGf2bysmboFJHIimTw987kNEOniERWtIK/vZ2Fjk4GZjRDp4hEV7SCH1jo66evmNfduyISWVcNfjPbZWbPmtnLZnbMzB5e5ZgHzCxvZkfC5fM1z71hZkfD/Qc3+wVcq8rgIP0zOd29KyKRlWjgmEXgM+5+2Mw6gUNm9oy7v1x33M/c/UNr/I73uful6yrpZhkaov/1IxzX3bsiElFXbfG7+zl3PxxuTwHHgZEbXbAbJbFDE7WJSLRdUx+/me0B9gPPr/L075nZC2b2AzN7e81+B542s0Nm9tCGS7pJksM76C0VKBSKzS6KiEhTNNLVA4CZbQMeBx5x90Ld04eB3e4+bWYfBL4D7A2fu9/dz5jZIPCMmf3W3X+6yu9/CHgIYHR0dAMvpTHx7cGQzrnzF4G7btjfERHZqhpq8ZtZkiD0v+HuT9Q/7+4Fd58Ot58CkmbWHz4+E64vAt8G7lvtb7j7Y+5+wN0PDIRfk3hDhGP5/cKFG/c3RES2sEZG9RjwVeC4uz+6xjHbw+Mws/vC33vZzDrCD4Qxsw7gA8BLm1X4DQmDP3ZRwS8i0dRIV897gE8AR83sSLjvc8AogLt/BfhPwJ+Z2SJQAj7q7m5mQ8C3wzohAXzT3X+4ya/h2oTBHx/XfD0iEk1XDX53/zlgVznmb4G/XWX/SWDfhkt3I4TB3z6xNUaXiojcbJG7c5dt21hItZGevNzskoiINEX0gt+MYk8fnXnN0Cki0RS94Afmegfo1wydIhJRkQz+xf6B4O5dzdApIhEUyeD3oSH6ZzRtg4hEUySDPzY0SG8xz+T0bLOLIiJy00Uy+JPDO0h4hdJ5fem6iERPJIO/bWQHAPNnzjW5JCIiN18kgz+zcxiAxXPnm1wSEZGbL5LBHx8OWvyaqE1EoiiSwa/5ekQkyqIZ/Nksi7E4ycv6cFdEoieawR+LUejq1URtIhJJ0Qx+YCbbR0dOE7WJSPRENvhLPX105SeaXQwRkZsussE/3z9Az/SkZugUkciJbPBX+gfpm8kxVdJ8PSISLZENfoYGaSsvkL+oD3hFJFoiG/zx7dsBmDl1tsklERG5uSIb/MmRIPhnxxT8IhItkQ3+9M4RABbOaqI2EYmWyAZ/x65gvp7Kec3XIyLREtng79q5gwoGFzVfj4hES2SDP9GWYjLTRUITtYlIxEQ2+AFyXb2kNF+PiERMpIN/qquXjIJfRCIm0sFf7OljW14TtYlItEQ6+Od6++mammx2MUREbqpIB/9C3wCZuRIUi80uiojITRPp4K8MDgZrfem6iERIpIPfwvl6iqc1bYOIREekgz+xoxr8Z5pcEhGRmyfSwd82HAT/3Fl19YhIdFw1+M1sl5k9a2Yvm9kxM3t4lWMeMLO8mR0Jl8/XPPegmb1iZifM7LOb/QKuR2ZnMF/P4jlN1CYi0ZFo4JhF4DPuftjMOoFDZvaMu79cd9zP3P1DtTvMLA58Gfh9YAz4tZk9ucrPNkV3Tyf5tg5cE7WJSIRctcXv7ufc/XC4PQUcB0Ya/P33ASfc/aS7zwPfAj6y0cJutmw6yaWOHmKar0dEIuSa+vjNbA+wH3h+lad/z8xeMLMfmNnbw30jwOmaY8ZYo9Iws4fM7KCZHRwfH7+WYm1YdzrJeEeWoYO/gM9/Hl54AVxfvi4ira3h4DezbcDjwCPuXqh7+jCw2933AX8DfOdaC+Luj7n7AXc/MDAwcK0/viGJeIyvvffjnB/dC1/4Atx9N+zdC3/xF/D881Cp3JRyiIjcTA0Fv5klCUL/G+7+RP3z7l5w9+lw+ykgaWb9wBlgV82hO8N9W8bxd7yL//3fHoNz5+Cxx4Lg/9KX4N3vht274c//HJ57DsrlZhdVRGRTNDKqx4CvAsfd/dE1jtkeHoeZ3Rf+3svAr4G9ZnabmaWAjwJPblbhN0M2neLS9Bw+MAB/+qfwgx8EX87yT/8E994Lf//38MADMDwMn/wk/OhHsLDQ7GKLiGyY+VX6tM3sfuBnwFGg2vfxOWAUwN2/YmafBv6MYARQCfgv7v6L8Oc/CHwJiANfc/cvXK1QBw4c8IMHD27oBV2rT/7zQX507AK393fwoX3D/OG+Ye4Y3LZ8wPR0UBk8/jh8//vB42wW3vc+6O+H7u5g6epaua7f195+U16PiESTmR1y9wMNHXu14G+Gmxn8hdkFvv/iOZ48cpZfvX4Zd3jbji4+vG8HH37nMLt6M8sHz87C00/DE0/AL38J+TwUClAqXf0PpVJBJZDNNrb09CxvDw5CPH7jToKI3PIU/Bt0sTDL94+e48kXzvKbUzkA9o9m+cN9w/yHd+xgsGuNVvv8fFABFApBZVCtEFbbzuchl4PJyWBd3Z6fX7tgiUTwecNtt62+DAxA0NO2dbgHr+3115eXN94I1qdOQTK5/M6o0aWrCzIZiN0iN5zPzwfvEGdmgmW1bbPg3WBbW7Cu3V5rnUhsrX/vchlefRWOHIGjR4N91cZLtQFTu+7u3poNmYmJ4HXULydPBmXesyf4f7hnz/KyezeMjgb/Nk2m4N8EpyeK/L8Xg0rg+LkCMYN3397Hh/cN8+Dbt9PTkdrcPzg7u1wR1C4TE3D69MoArR/u2tERXIS3375cGdRemNns5geFe1CJjY1dGezVpVA3+CubDco0OhqERW1lWK0gr3Y9mgWvd9u2YOnsXN5e7XFHR/Cfsnaphuha++JxmJpaWZlXt9d7XB/si4ube87rxWLB+aiua7fr1+l0cH285S1wxx0r1729jV8f09NBuB85srwcPbr8rjeZDP4Nr/bau7pWVgx33BF8lvbe98LOndd1WtZVKsGJE8uh/sory9uXa76UKR4PzteddwbrQiG4vt98M/j/WD/YY3h4uVKorvv6rry26q+92seJRu6nXZuCf5OduDjF9144x/deOMvJSzMkYsa/3dPLbQMd7OrJsLMnza7eDLt60vR2pLAb3Rqbnl4ZsidPrgzc6emVx3d1BRdj/YVZ3Vf/jqFchvPn4cyZYBkbW96ufVz/PQaZzHKlU/uOpPo4m13/dVUqQdlzuSsrhXw+COPp6ZVL/b7q46mpGzccNx5f+S6kulQrmvr1etvuMDcXVPzVde32as8tLAQ/5x68xtXW9fump4Pr5He/C/79amWzq1cIw8Pw2msrQ/6115Yr554e2L8/GAZdXd761iDAisXgnWz1ne3Vtl96KXgMwd9+4IHlZaMVQT4Pv/kNHD4Mhw4F61deWdm4GBkJwr1+ue22oBJbzeJicP1XK4I33li5ferUxir9eDwoz5tvXvvPouC/YdydY2cLfO/Fs/zyd5c5PVFksrhyhE8mFQ8qgpoKYWe4PdqXoat9jYtp8woZtFyqF2L1Yqxd5/MrfyadDiqAzk44ezYY2lofmslkcFHWLjt3ButqsG+lLqdqoM7MBOvqUg3R9R6XyysDvTbgu7uDVtpWeZ0bUSoFDYQTJ4Lld79bXr/xxupDl2+/fWXA33138O+/WeehXA7eOfzkJ8Hy3HPXVhFMTATBXl0OHQpeU9XOnXDPPUFF9ba3BeG+d29QCW+2cjn4P5TLrby+Vtuu39feDn/1Vxv6swr+m2h6bpGxySKnJ0pL69OTRcYmS4xNFJmaW1nzD3a2cedQJ3uHtnHnUOfS9g2vEGrlcsuVQm2FMDW1drj39986feuycQsLQYv1xImgVbt3L7zznUGFdzM1UhHs3h3cbX/oUHANV+3ZE4T8vfcG63vuCQZItDgF/xbh7uRLC4xNljg9UeTNiSKvXZjmtYtTvHZhmtLCcstqR3c7e4c6uXNw21JlsHeok21t19fvJ9IS1qoI7rhjOdzvvTdo0ff1Nbu0TaHgvwVUKs7YZIlXL0zxalgRvHJ+ihPj08wvLnezjGTTpFNxyhVfubhTqTiLlWBd9pXbcTPak3HaErFgnYzRnojTnoyt2B8sMdoSwb5E3IjHYiRiRjxmK9fxK/fHzJhfrDBfrgTrxQpzi+VgXa4wt3Dlc+WK85aBbewf7eGe3VkGO3WPg1yjcjnosroRXTV1csV50qk4bYktOBKphoL/FlauOKcmirx6YYrXLkxx4uI0C2UnFjPiBrG60I2H2/FwOxZul92ZW6gwu1hmdqEcbC+UmVsM1sH+IIhna56rViabIZWI0RaP0ZaMkYrHSCWCJWbGyfEZ5stBBberN809oz3cu7uHe0Z7eOv2ThLxrduttFCukCsuMFmcZ3JmnsniAlOzC/R3tjHam2Ekm6Y9ubVDol6l4kzNLTI1u8DU7CJTs4vMzC2yUK5QcadcYamxUdvwWLkv+D3z5QpzYSVfrfjnFmoaBIv12xViBulUgkwyTiYVJ50K1plUIthOVvcllp7PppPs6e9gsLNt0wZUTM8tcnQszwtjOV44HSxn87MADHW1sasnszSQY2dvJnycZkd3mnisuZ/7KPjlurg7FYfFSoVyWBGUy+G64iv2Vyq+FOipeIy2ZJxUPEYybuv+Z5xdKHPsbJ7Db+Y4fGqSw6cmuVCYAyCdjPPOnd1LFcE9u3vo3ezhs3Wvt1BaZCxX5GxulvOFWSZn5pmYmSdXDII9V5xnojhPbmbhis9tVrO9q51dvdXRXhlGe4PAGO3NMNjZRuwGhsRCucL41Bzn8rNcKMxyLj/Lpek5CqVqqC+HeyHcnm7gNV2rVCJG29ISvKNM1TyubqcSMSruFOeDRkpxvkxpPlgX5xcpLZRZKK+dUx2pOHv6O7htlSWbWfu6mV+s8Mr5KY6EIf/iWI7XLk4vDfrZ3Zdh384s/2aki+J8eanLdmyyxLl8idr2USJmDGfTSwM7dvWm6e1oW37HXffOe7V1WyJ2XRWYgl9uOe7OmVyJw6dyHH5zkt+cmuTY2cLSu4/b+jvY05chm0nRnU7SnU6SzdSuUyv2JWveMSyUK1wozHI2N8uZMNzP5EqcmSxxNhcsM/NXjmTpbEuQ7UjSk0mRzaTozSTJZlL0ZFL0dFS3g+c72xNcnJrj9ESRUxPhh/wTRU5PFjlfmF0xgjCViC0FRG9HaqlFG7RkE3Ut3jjpZGJ5OxVnfrHC+cIs5/Ozy+ua7fHpuStuh0jGja72JJ3tCTqX1rXbSbrq9m1rS5CMx2reWbK0Xftuc2nbjFiMpUbAZg5rXihXaiqERYrzZSZm5nnj8gyvX1peTk8UVwRyTyYZXDv9Hdze30H/tjZ+e36KF8ZyHDtbWOpW7etIsW9Xln07s+zb1c07d2bXbWzML1Y4ly8tDeYI/q2XB3hcmp7b0Osc7m7nF3/5/g39rIJfWsLsQpmjZ/IcCiuCs7lZcqV5csWglbqejlScbCZFxZ0LhVnqe6/6OlIMZ9MMZ9sZzqYZCZfhbJod3e1kMylSic3pbppbLHNmssTpyRKnJoqMVSuHySL50sJS67a0UN7Q10F0tSfY0Z1me3c727vag3V16WpnR3c73enkjb+/ZAuYX6xwerLI6+NhZXB5Zmn7fCHoskkn47xjpJt9u7qXwn5nT3pTz09pvkxhdmFF9+rSeqkLdrmrtbpOxY1P//u9G/qbCn5peeWKUygtkC8tkCsFXTH58HG+WN0X3GMxkm1npCcdBn2a4e7gA/Otxt2ZXagstWhLCzXdHfPLXSCJuK0I+UxKI78aUZxfZHxqjpFsekt/hrRR1xL8umLklhSPGT0dqc2fOqOJzIx02J0TzQGJN1YmlWB3nyIPrvGrF0VE5Nan4BcRiRgFv4hIxCj4RUQiRsEvIhIxCn4RkYhR8IuIRIyCX0QkYrbknbtmNg5s7PvHoB+4tInFuVXpPAR0HgI6D4FWPg+73X2gkQO3ZPBfDzM72Ohty61M5yGg8xDQeQjoPATU1SMiEjEKfhGRiGnF4H+s2QXYInQeAjoPAZ2HgM4DLdjHLyIi62vFFr+IiKyjZYLfzB40s1fM7ISZfbbZ5WkmM3vDzI6a2REzi8w32pjZ18zsopm9VLOv18yeMbPXwnVPM8t4M6xxHv6HmZ0Jr4kjZvbBZpbxZjCzXWb2rJm9bGbHzOzhcH/krol6LRH8ZhYHvgz8AXAX8DEzu6u5pWq697n73REbuvZ14MG6fZ8Ffuzue4Efh49b3de58jwA/K/wmrjb3Z+6yWVqhkXgM+5+F/Bu4FNhLkTxmlihJYIfuA844e4n3X0e+BbwkSaXSW4yd/8pMFG3+yPAP4bb/wj8x5taqCZY4zxEjrufc/fD4fYUcBwYIYLXRL1WCf4R4HTN47FwX1Q58LSZHTKzh5pdmCYbcvdz4fZ5YKiZhWmyT5vZi2FXUKS6N8xsD7AfeB5dEy0T/LLS/e5+D0HX16fM7N81u0BbgQdD2KI6jO3/AG8B7gbOAf+zucW5ecxsG/A48Ii7F2qfi+o10SrBfwbYVfN4Z7gvktz9TLi+CHyboCssqi6Y2Q6AcH2xyeVpCne/4O5ld68Af09ErgkzSxKE/jfc/Ylwd+SviVYJ/l8De83sNjNLAR8FnmxymZrCzDrMrLO6DXwAeGn9n2ppTwJ/Em7/CfDdJpalaapBF/ojInBNmJkBXwWOu/ujNU9F/ppomRu4wuFpXwLiwNfc/QtNLlJTmNntBK18gATwzaicCzP7F+ABghkYLwD/HfgO8H+BUYIZX/+zu7f0B59rnIcHCLp5HHgD+GRNP3dLMrP7gZ8BR4FKuPtzBP38kbom6rVM8IuISGNapatHREQapOAXEYkYBb+ISMQo+EVEIkbBLyISMQp+EZGIUfCLiESMgl9EJGL+P7wKb3E5dUNgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
