{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /home/jkane021/.local/lib/python2.7/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 4573338\n",
      "train len:  4116004\n",
      "test len:  457334\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = './shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ot tell vat is dat: but it is tell-a me dat\n",
      "you make grand preparation for a duke de Jamany: by\n",
      "my trot, dere is no duke dat the court is know to\n",
      "come. I tell you for good vill: adieu.\n",
      "\n",
      "Host:\n",
      "Hue and c\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "learning_rate = 0.01\n",
    "model_type = 'rnn'\n",
    "print_every = 50\n",
    "plot_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    batch_size = input.size(0)\n",
    "    chunk_len = input.size(1)\n",
    "    hidden = rnn.init_hidden(batch_size)\n",
    "    rnn.zero_grad()\n",
    "    for cur_chunk in range(chunk_len):\n",
    "        output, hidden = rnn(input[:, cur_chunk], hidden)\n",
    "        loss += criterion(output.view(batch_size, -1), target[:, cur_chunk])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return (loss.data[0]/chunk_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 41s (50 1%) train loss: 2.5020, test_loss: 2.5177]\n",
      "Wheyathar is he Ithaif were diseisher pan:\n",
      "\n",
      "Ifous myoueve tacee y hak sho myte t fre, les br\n",
      "Thithifot \n",
      "\n",
      "[1m 21s (100 2%) train loss: 2.4953, test_loss: 2.4817]\n",
      "Wh welabenoure ILENGoursthang men hef o y t y s wanth n tomol oug ony I tsthen winngowoomy f I ilalshe \n",
      "\n",
      "[2m 2s (150 3%) train loss: 2.4997, test_loss: 2.4856]\n",
      "Whismed bucher meme hour s'se werce ealdill ced t\n",
      "Andshe.\n",
      "S:\n",
      "\n",
      "Wis wnd t wisthince htctoule loutred whe \n",
      "\n",
      "[2m 43s (200 4%) train loss: 2.4732, test_loss: 2.4960]\n",
      "Wheak thinthed st,\n",
      "\n",
      "se.\n",
      "\n",
      "He lo at ife ps y bord wisthue fatoho gino whedledse.\n",
      "\n",
      "BOnckes menthals tothy \n",
      "\n",
      "[3m 24s (250 5%) train loss: 2.4828, test_loss: 2.4945]\n",
      "Whavent iorristinthas, lin'stem sthe\n",
      "Thasie--\n",
      "STRMalllsu'de, s fin be thint ds t greme couelt f hat ke \n",
      "\n",
      "[4m 5s (300 6%) train loss: 2.4813, test_loss: 2.4948]\n",
      "Whie thinear:\n",
      "NESS:\n",
      "Buby har as I'le; re avie br ay! hu.\n",
      "\n",
      "\n",
      "Mioth; the t s?\n",
      "PH:\n",
      "Whes,\n",
      "\n",
      "RUCLOMand tousig \n",
      "\n",
      "[4m 46s (350 7%) train loss: 2.5127, test_loss: 2.4880]\n",
      "Whawicour ther I'd nardor ge pey s\n",
      "Wisatour onth sandeat.\n",
      "BELERKI cher ithan hayenoom,\n",
      "G d my whel he  \n",
      "\n",
      "[5m 27s (400 8%) train loss: 2.4974, test_loss: 2.4955]\n",
      "Wheivisplolad s and nt d ber my prel s athe oro oursep thovanoma thy d! ngelokerod hir aneld thegholda \n",
      "\n",
      "[6m 8s (450 9%) train loss: 2.4905, test_loss: 2.5025]\n",
      "Whatous mpiso tle gr Ande al y t ad meanore or'sth las s.\n",
      "War witinge hes at ws, tl thadote ircare s m \n",
      "\n",
      "[6m 50s (500 10%) train loss: 2.4714, test_loss: 2.4957]\n",
      "Whe ssavenou, the ir hes\n",
      "TE:\n",
      "Would ow ate wn d ton os nourane, is my t t w t sthis d yo a knde bous at \n",
      "\n",
      "[7m 31s (550 11%) train loss: 2.4899, test_loss: 2.4868]\n",
      "Whoure fo my le r\n",
      "\n",
      "HO:\n",
      "MAuseat loveatheaf s thatl whald hel wed charikingerlay, m tan o by indend ke o \n",
      "\n",
      "[8m 12s (600 12%) train loss: 2.4969, test_loss: 2.4695]\n",
      "Whe;\n",
      "He ise r sthe constubed, aige\n",
      "anging fe pank socenghell, arain I t my, bre ns t beare ato myoulou \n",
      "\n",
      "[8m 53s (650 13%) train loss: 2.4887, test_loss: 2.4737]\n",
      "Whan altho hefo, he iroughein s ang wairowalipe hay IDou t rerertong athed yours! yothey per me,\n",
      "\n",
      "\n",
      "\n",
      "Di \n",
      "\n",
      "[9m 34s (700 14%) train loss: 2.4809, test_loss: 2.5128]\n",
      "Wh hanon OCESe hendanchert r f wes thandind I st tinondere lit ald t seale fo llit'The.\n",
      "PUSAMES:\n",
      "\n",
      "TEn  \n",
      "\n",
      "[10m 14s (750 15%) train loss: 2.4893, test_loss: 2.4947]\n",
      "Whiofore Yonn t t hainelly ath p me seeethe tha ar burentious, alorerereayof twans f d ce t tcure meal \n",
      "\n",
      "[10m 55s (800 16%) train loss: 2.4786, test_loss: 2.4819]\n",
      "Whor, sthan inyom br e.\n",
      "BALAne it wist thant toter aind mabeme m, aith bland her'etre brofahig ne\n",
      "CEND \n",
      "\n",
      "[11m 36s (850 17%) train loss: 2.4905, test_loss: 2.5091]\n",
      "Whe tethitoust, stiesst:\n",
      "K:\n",
      "Juth wier, the thad qurerct thit angh tan han and ch t tof baur, n w, oren \n",
      "\n",
      "[12m 17s (900 18%) train loss: 2.4757, test_loss: 2.4900]\n",
      "Wheeat hthountin the imale thay f me s you he t owofengathe t pr:\n",
      "Wrout wathee ceme ou keaghe grcay ow \n",
      "\n",
      "[12m 58s (950 19%) train loss: 2.4857, test_loss: 2.4881]\n",
      "Whar h I an aive athinghe acu t belowavegr.\n",
      "Whte.\n",
      "N:\n",
      "Wheate tha ghe pris bee whing f ble nesundouse me \n",
      "\n",
      "[13m 39s (1000 20%) train loss: 2.4906, test_loss: 2.5015]\n",
      "Whe s, an, fou, t st avou\n",
      "Tond nders s, an's thiso d t his,\n",
      "Thids:\n",
      "I pr stous Mand s, anory re t\n",
      "\n",
      "\n",
      "I w \n",
      "\n",
      "[14m 20s (1050 21%) train loss: 2.4682, test_loss: 2.4890]\n",
      "Whore ourg monon t he by bou;\n",
      "\n",
      "Ithod mpthare y saromomas outo t s trle\n",
      "\n",
      "romyowoun thachecthere Fof be  \n",
      "\n",
      "[15m 2s (1100 22%) train loss: 2.4694, test_loss: 2.4974]\n",
      "Whilit tasir e I w ptow onsiringonave tathite t usouchtthe d gh st s re fo trerind sin wato searre-eth \n",
      "\n",
      "[15m 43s (1150 23%) train loss: 2.4672, test_loss: 2.4852]\n",
      "Whe th asil themair we ed f t thaybathene wol oou ars buthy,\n",
      "WAy. s S:\n",
      "Courongheamparitho woaveand s i \n",
      "\n",
      "[16m 24s (1200 24%) train loss: 2.4866, test_loss: 2.4760]\n",
      "Wherar fr s I s fual Muru ferere hod s, k.\n",
      "BEDIINICANAy y ticrealowind;\n",
      "\n",
      "STRel--d ad d he a thayos the \n",
      "\n",
      "[17m 5s (1250 25%) train loss: 2.4769, test_loss: 2.4950]\n",
      "Whay ho y ateremithine hire tond, n esthatite winde aiera t y, abender s wor buse\n",
      "Mamout o y the\n",
      "I f k \n",
      "\n",
      "[17m 46s (1300 26%) train loss: 2.4846, test_loss: 2.4829]\n",
      "Whin! s beare. ngak; f ll wid aren:\n",
      "MARISemer! he lergor te fist t ad yo atie the my.\n",
      "Tisurers y myome \n",
      "\n",
      "[18m 27s (1350 27%) train loss: 2.5035, test_loss: 2.4908]\n",
      "Whod. s o withit n we heat, ulithande thel thare t, m he, d IULoliche cer y be MI bl llong mand bushed \n",
      "\n",
      "[19m 8s (1400 28%) train loss: 2.4718, test_loss: 2.4984]\n",
      "Wh in t welloo bu hat in s il y t poothearr n ou t wnthe ang te ers arga il t ath aremy be farireandes \n",
      "\n",
      "[19m 49s (1450 28%) train loss: 2.4834, test_loss: 2.5017]\n",
      "Whay cacouowh,\n",
      "Anofl'l fono hethind manof me, LVI pakiendist wit weifry hathechers anores,\n",
      "ORCAI he yo \n",
      "\n",
      "[20m 30s (1500 30%) train loss: 2.4873, test_loss: 2.4720]\n",
      "Whakerd me biten thunonde hereamear mer t byond the Bofory the ber PRDAnd ame inealorer oust'Theshe ho \n",
      "\n",
      "[21m 11s (1550 31%) train loss: 2.4792, test_loss: 2.4899]\n",
      "Whe meenant sw IOLUS:\n",
      "Bo t, withale tr sildounes ft theanoueder my war te dre ayowinowhoth g d. herd s \n",
      "\n",
      "[21m 52s (1600 32%) train loss: 2.4696, test_loss: 2.4833]\n",
      "Wherele d cat he s\n",
      "ML:\n",
      "\n",
      "\n",
      "I wontha bullleeedgr thobloutl uroonth y ate d ch ot hatour'ther prantof wor, \n",
      "\n",
      "[22m 34s (1650 33%) train loss: 2.4927, test_loss: 2.4770]\n",
      "Whaily theaverg m hagr, thang thy t t sistod we, f n th, m guetre y? fry hinean thornd nd tot,\n",
      "Wher he \n",
      "\n",
      "[23m 16s (1700 34%) train loss: 2.4780, test_loss: 2.4857]\n",
      "Whitearng e, heat; othe w earerst thesheng a keve I my foun mpanothacongengeghicor the ar wong l--t he \n",
      "\n",
      "[23m 57s (1750 35%) train loss: 2.4776, test_loss: 2.4972]\n",
      "Whefomere, you was, hat wot wouise heroule ifouse s d t wit ar ak! s t y hit d g atowour tcer ro gris\n",
      " \n",
      "\n",
      "[24m 39s (1800 36%) train loss: 2.4847, test_loss: 2.4860]\n",
      "Whis ar f theryog bueayore d g wn\n",
      "Whathand s f s.\n",
      "SY:\n",
      "CES: my le mel wast ath iers FRIUpllincthed?\n",
      "Tha \n",
      "\n",
      "[25m 20s (1850 37%) train loss: 2.4750, test_loss: 2.5087]\n",
      "Why orse ald! ingesond wind my t ncang ser se t susen DEROfinoreplld he me thamy st omyors f k ghe me  \n",
      "\n",
      "[26m 1s (1900 38%) train loss: 2.4660, test_loss: 2.5070]\n",
      "Whedofanourthoo mesth athale frs, hehinon nend scuthe aindes athe the ang bran:\n",
      "ALLI s th d br nghe ll \n",
      "\n",
      "[26m 42s (1950 39%) train loss: 2.4822, test_loss: 2.4827]\n",
      "Wharpinderiritu oulont fe woume make suthiomurinolll byon thyorsiooursthouthe thetheel herero l thise  \n",
      "\n",
      "[27m 23s (2000 40%) train loss: 2.4784, test_loss: 2.4814]\n",
      "Wheluby cke\n",
      "\n",
      "Whisthen fre\n",
      "O, he porow, pronoule,\n",
      "Whis way, thomes imide esthies d, luce,\n",
      "NRI ct ldyor  \n",
      "\n",
      "[28m 4s (2050 41%) train loss: 2.4880, test_loss: 2.4923]\n",
      "Whing cengich spave alind cane, de the y or dlos ild:\n",
      "JO: s fowatyowime ano nord st mavit mey yor ast  \n",
      "\n",
      "[28m 45s (2100 42%) train loss: 2.4977, test_loss: 2.4950]\n",
      "Wheren d to intovouno hiloot anirtowasupony, WARELI fo brit andirwrer my k ase sturthinend ce t chithe \n",
      "\n",
      "[29m 26s (2150 43%) train loss: 2.4796, test_loss: 2.4862]\n",
      "Whers se men miche in mld t.\n",
      "\n",
      "M:\n",
      "\n",
      "D:\n",
      "SThe f I'd by me he her mpr.\n",
      "Me atireais t leld here foou,\n",
      "Here I \n",
      "\n",
      "[30m 7s (2200 44%) train loss: 2.4871, test_loss: 2.4919]\n",
      "Whern? t athe\n",
      "\n",
      "Tr nde'souts ger dun g t watisha wor r s s, beat e be. f\n",
      "\n",
      "Tho illorimyor werathar s f t \n",
      "\n",
      "[30m 49s (2250 45%) train loss: 2.4745, test_loss: 2.4830]\n",
      "Whe il wourend and ssees nerst theivee yses, tit omime, his f he ht, noul thichese' th,\n",
      "\n",
      "\n",
      "Fowe:\n",
      "ARDed  \n",
      "\n",
      "[31m 30s (2300 46%) train loss: 2.4777, test_loss: 2.4939]\n",
      "Whe gheanor be imon y t.\n",
      "\n",
      "D:\n",
      "thanoun n nd iomos, ce?\n",
      "\n",
      "\n",
      "ANou y meen yontir, ald messe chease f n t pre  \n",
      "\n",
      "[32m 11s (2350 47%) train loss: 2.4865, test_loss: 2.4774]\n",
      "Whistedead! mit I'ld s ane ther s in d t onou heallllll whe mee an, tomerre yot wnd gshant staf thith  \n",
      "\n",
      "[32m 52s (2400 48%) train loss: 2.4761, test_loss: 2.4852]\n",
      "Whisallantis IA:\n",
      "Thele th llchaneral CEAnjuthoralitherat his d f\n",
      "\n",
      "S:\n",
      "GANoue thal'ser nte. tind; w n sw \n",
      "\n",
      "[33m 33s (2450 49%) train loss: 2.4666, test_loss: 2.4820]\n",
      "Wharsethe be\n",
      "Burer an\n",
      "RI:\n",
      "MLAnd i'sthel he wathisen hethe bind beme htor t t achano t, f h, t thand,\n",
      "\n",
      " \n",
      "\n",
      "[34m 14s (2500 50%) train loss: 2.4873, test_loss: 2.4934]\n",
      "Whf n ae is me adelis, ve sond bof caveas athy fofar spereme ta s he ghourake y ss at, thatrdveall'los \n",
      "\n",
      "[34m 56s (2550 51%) train loss: 2.4772, test_loss: 2.4865]\n",
      "Wher u owind the, myousthat goust hercou the w arved,\n",
      "\n",
      "\n",
      "\n",
      "He o bos m,\n",
      "Me temaiand ofel d shanore me har \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35m 37s (2600 52%) train loss: 2.4744, test_loss: 2.4908]\n",
      "Whes t yove f llo t th hanequthon s llelot cor liscoulofow a oman mma t the:\n",
      "\n",
      "SThe a,\n",
      "Sot,\n",
      "PHaispareso \n",
      "\n",
      "[36m 18s (2650 53%) train loss: 2.4808, test_loss: 2.5007]\n",
      "Whas y,\n",
      "\n",
      "TERDORLShites os thas us t in an teisers thithid chin ffoncod, athe theeay Andou cr'd nd foup \n",
      "\n",
      "[36m 59s (2700 54%) train loss: 2.4936, test_loss: 2.4928]\n",
      "Whise, by;\n",
      "An ckfllie wirorthis w gr theithisenthiram\n",
      "\n",
      "\n",
      "Whe ce shischatio, cllollor t itoveilangof d d \n",
      "\n",
      "[37m 40s (2750 55%) train loss: 2.4935, test_loss: 2.5045]\n",
      "Whe tinos thed in arwis\n",
      "Here rdad trie hofon IONThe an hars, mmalind he se thed s per'd t f onallo t n \n",
      "\n",
      "[38m 21s (2800 56%) train loss: 2.4775, test_loss: 2.4772]\n",
      "Wh ar\n",
      "TREVA toughen! thowheantowit rin thare torshen?\n",
      "\n",
      "\n",
      "t.\n",
      "FFin, t;\n",
      "FOLS:\n",
      "ILI w me theanckeas wigh the \n",
      "\n",
      "[39m 2s (2850 56%) train loss: 2.4607, test_loss: 2.5009]\n",
      "Whemy pis s atoitorershan IUI masththels the whowherer trill chorso fe be jure at,\n",
      "Naren yout amenhis  \n",
      "\n",
      "[39m 43s (2900 57%) train loss: 2.4824, test_loss: 2.4860]\n",
      "Whamy! miss o pitoure,\n",
      "Wh,\n",
      "S:\n",
      "Agle, lkiolldd theeane\n",
      "NGROfed\n",
      "\n",
      "\n",
      "It s,\n",
      "T:\n",
      "Whie he.\n",
      "k pend t n ay akn sts \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f7697cd3fcd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training for %d epochs...\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mload_random_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f0cb56debc0a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rnn, input, target, optimizer, criterion)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcur_chunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_chunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_chunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gan/rnn/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m#          YOUR CODE HERE          #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m####################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hx, batch_sizes)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvariable_length\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             dropout_ts)\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network\n",
    "torch.save(classifier.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
