{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --ignore-installed unidecode\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 4573338\n",
      "train len:  4116004\n",
      "test len:  457334\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = './shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst of words.\n",
      "\n",
      "IAGO:\n",
      "Good my lord, pardon me:\n",
      "Though I am bound to every act of duty,\n",
      "I am not bound to that all slaves are free to.\n",
      "Utter my thoughts? Why, say they are vile and false;\n",
      "As where's th\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "# chunk_len = 300\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 200\n",
    "n_layers = 2\n",
    "learning_rate = 0.001\n",
    "model_type = 'rnn'\n",
    "print_every = 100\n",
    "plot_every = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    batch_size = input.size(0)\n",
    "    chunk_len = input.size(1)\n",
    "    hidden = rnn.init_hidden(batch_size)\n",
    "    rnn.zero_grad()\n",
    "    for cur_chunk in range(chunk_len):\n",
    "        output, hidden = rnn(input[:, cur_chunk], hidden)\n",
    "        loss += criterion(output.view(batch_size, -1), target[:, cur_chunk])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return (loss.data[0]/chunk_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2m 20s (100 2%) train loss: 2.0957, test_loss: 2.1101]\n",
      "Whince to to loed,\n",
      "That in to efare fort your the of with nok but; thet he thou siunt or well a be the \n",
      "\n",
      "[4m 42s (200 4%) train loss: 1.8665, test_loss: 1.8972]\n",
      "Whach thy with that loud lit in a, as to him a palice?\n",
      "To him proan's a madies:\n",
      "Doim do not.\n",
      "\n",
      "ORING\n",
      "Ma \n",
      "\n",
      "[7m 2s (300 6%) train loss: 1.7656, test_loss: 1.8012]\n",
      "Why, Grows amporty at\n",
      "at's exfirds, old I an with though grod beat they hath as but as my garoods me y \n",
      "\n",
      "[9m 23s (400 8%) train loss: 1.6941, test_loss: 1.7426]\n",
      "Whom I\n",
      "hear's vucan of living.\n",
      "\n",
      "GERAND:\n",
      "Come, when the deaky here barred my down\n",
      "That and I ome know a \n",
      "\n",
      "[11m 46s (500 10%) train loss: 1.6538, test_loss: 1.6836]\n",
      "Why: I less he hath so makes, ride am water and present fortenes'd, no profition disonard of his comes \n",
      "\n",
      "[14m 8s (600 12%) train loss: 1.5941, test_loss: 1.6409]\n",
      "What you between of it.\n",
      "\n",
      "EDRONICUS:\n",
      "I will never remember him,\n",
      "And I do neger in the littless promised \n",
      "\n",
      "[16m 31s (700 14%) train loss: 1.5911, test_loss: 1.6189]\n",
      "Who.\n",
      "\n",
      "MELENA:\n",
      "No, for a scall out free in the give of rue becomeful invoste underfuage,\n",
      "For which she  \n",
      "\n",
      "[18m 55s (800 16%) train loss: 1.5502, test_loss: 1.6030]\n",
      "Who play with recuse with thy heart with them in her supped for an is a riple, and ho!\n",
      "\n",
      "MEMENA:\n",
      "Nay, b \n",
      "\n",
      "[21m 18s (900 18%) train loss: 1.4936, test_loss: 1.6033]\n",
      "Why Edward you: there words the earnous candred, never to him to the mole\n",
      "it this is my more their foi \n",
      "\n",
      "[23m 42s (1000 20%) train loss: 1.5151, test_loss: 1.5730]\n",
      "Who a store a bulless of me with man.\n",
      "\n",
      "EDCULIA:\n",
      "What play you to make they may and the French's time\n",
      "I \n",
      "\n",
      "[26m 6s (1100 22%) train loss: 1.4983, test_loss: 1.5854]\n",
      "What heaven next gone: but oursoles which was new the bare.\n",
      "\n",
      "First Gedrcation of the Castalless on my  \n",
      "\n",
      "[28m 28s (1200 24%) train loss: 1.5015, test_loss: 1.5702]\n",
      "What you, sure no more,\n",
      "You are green to Caison not, you all you here and the man.\n",
      "\n",
      "KING HENRY VI:\n",
      "The \n",
      "\n",
      "[30m 51s (1300 26%) train loss: 1.4808, test_loss: 1.5585]\n",
      "What is thee saudder.\n",
      "\n",
      "First Soldier to heald.\n",
      "\n",
      "JULIET:\n",
      "Both exchange the thought the night\n",
      "That the d \n",
      "\n",
      "[33m 14s (1400 28%) train loss: 1.4696, test_loss: 1.5665]\n",
      "Whow he fell. How do it cannot be Lady and jour violors' have her cardon for the war here.\n",
      "\n",
      "EDWARD:\n",
      "It \n",
      "\n",
      "[35m 37s (1500 30%) train loss: 1.4274, test_loss: 1.5363]\n",
      "Where cannot be burning right:\n",
      "I pray you so grown.\n",
      "\n",
      "KING RENCENTIO:\n",
      "I shout.\n",
      "\n",
      "ACHILLES:\n",
      "I will be my  \n",
      "\n",
      "[38m 1s (1600 32%) train loss: 1.4094, test_loss: 1.5202]\n",
      "When like the honour,\n",
      "For that had his pelecation. But the furist to-night.\n",
      "\n",
      "MOTH:\n",
      "O think\n",
      "In honour b \n",
      "\n",
      "[40m 26s (1700 34%) train loss: 1.4218, test_loss: 1.5085]\n",
      "What they speak as we can Marry of Gods,\n",
      "To the meet;\n",
      "My last been to this; for I be expeat. Now not m \n",
      "\n",
      "[42m 51s (1800 36%) train loss: 1.4331, test_loss: 1.5374]\n",
      "Whow have I struck on the heart of you?\n",
      "\n",
      "PROTEUS:\n",
      "Thou canst loved what I may unloved mink in hands\n",
      "Fr \n",
      "\n",
      "[45m 15s (1900 38%) train loss: 1.4354, test_loss: 1.5416]\n",
      "Who with this,\n",
      "Thou to the king:\n",
      "O will but for compliness shall be the tears blood\n",
      "And say the tears\n",
      " \n",
      "\n",
      "[47m 38s (2000 40%) train loss: 1.4296, test_loss: 1.5239]\n",
      "What he the respleed\n",
      "The stop out and there is thou offence; a, if there, like a duke hold to do you,  \n",
      "\n",
      "[50m 2s (2100 42%) train loss: 1.4286, test_loss: 1.5045]\n",
      "Which this as of a better on him out.\n",
      "\n",
      "OCKING HENRY VI:\n",
      "A thinding and a monting conceunt right with i \n",
      "\n",
      "[52m 23s (2200 44%) train loss: 1.3882, test_loss: 1.5375]\n",
      "What do 'tis not to the world in the song.\n",
      "\n",
      "MISTRESS PAGE:\n",
      "I will keep away, he comes in the field in  \n",
      "\n",
      "[54m 43s (2300 46%) train loss: 1.4050, test_loss: 1.5050]\n",
      "Why, not stood to us't! thou wast did not a tooth\n",
      "Not a long toward I'll must not alone\n",
      "In thing of hi \n",
      "\n",
      "[57m 5s (2400 48%) train loss: 1.4281, test_loss: 1.4967]\n",
      "What the gods were you to be thy tears, as I do.\n",
      "\n",
      "TRANIO:\n",
      "What is a sun and breath in heaged for a sak \n",
      "\n",
      "[59m 27s (2500 50%) train loss: 1.4098, test_loss: 1.4906]\n",
      "What monster'd the duke.\n",
      "\n",
      "FALSTAFF:\n",
      "It is their wrong.\n",
      "\n",
      "KING HENRY IV:\n",
      "O, so a give him.\n",
      "\n",
      "POINS:\n",
      "I'll  \n",
      "\n",
      "[61m 49s (2600 52%) train loss: 1.3751, test_loss: 1.4774]\n",
      "Whe think'st think that vexides.\n",
      "\n",
      "ADRIAN:\n",
      "I have hold his boseath of himself:\n",
      "For what I was their sta \n",
      "\n",
      "[64m 10s (2700 54%) train loss: 1.4266, test_loss: 1.5134]\n",
      "What thou there's to the once strangely,\n",
      "'Tis the most hundred thought theresome than these flowers.\n",
      "\n",
      " \n",
      "\n",
      "[66m 31s (2800 56%) train loss: 1.4086, test_loss: 1.4661]\n",
      "Why fear and that is a guilt, and the garden enconounce the partily?\n",
      "\n",
      "PaGE:\n",
      "Is the state; it is a Lord \n",
      "\n",
      "[68m 52s (2900 57%) train loss: 1.3984, test_loss: 1.5013]\n",
      "What, what a hand\n",
      "In which in such a man'd;\n",
      "And that comes on your peace bold of it garden, and you la \n",
      "\n",
      "[71m 15s (3000 60%) train loss: 1.3602, test_loss: 1.4631]\n",
      "Who nothing wouldst we do not with me,\n",
      "So do you not are but our slack done to endage of discourtesy,  \n",
      "\n",
      "[73m 37s (3100 62%) train loss: 1.4225, test_loss: 1.4970]\n",
      "What ensinus thrust thee, I beseech you be sack of break! consider'd judgment of a mouth\n",
      "The reckation \n",
      "\n",
      "[76m 0s (3200 64%) train loss: 1.3777, test_loss: 1.4607]\n",
      "When you lord.\n",
      "\n",
      "CORNWALL:\n",
      "It born of my son;\n",
      "But to whom a heart is beew?\n",
      "\n",
      "MARK ANTONY:\n",
      "Alas, of the h \n",
      "\n",
      "[78m 21s (3300 66%) train loss: 1.3659, test_loss: 1.5021]\n",
      "Who wish his master to my very scuried hangs:\n",
      "What I mean, that what he's worth for the princes of my  \n",
      "\n",
      "[80m 42s (3400 68%) train loss: 1.3761, test_loss: 1.4671]\n",
      "What she come, heaven so seek to the flowers crown for an answer them i' the embrach him.\n",
      "\n",
      "LAFEU:\n",
      "So f \n",
      "\n",
      "[83m 5s (3500 70%) train loss: 1.3633, test_loss: 1.4881]\n",
      "What adversumber, and to heaven from him?\n",
      "\n",
      "ORLANDO:\n",
      "I thank you?\n",
      "\n",
      "HAMLET:\n",
      "Now, now I have a seamon hav \n",
      "\n",
      "[85m 27s (3600 72%) train loss: 1.3456, test_loss: 1.4731]\n",
      "What justing to usual complexion never, is indeed.\n",
      "\n",
      "EROS:\n",
      "An honour medy 'good Gremiar!\n",
      "A partfully pr \n",
      "\n",
      "[87m 51s (3700 74%) train loss: 1.3565, test_loss: 1.4680]\n",
      "Why and thou must be well in me too.\n",
      "\n",
      "PROGURET:\n",
      "I know the heart of blood; slawn, I have capp thee? wh \n",
      "\n",
      "[90m 13s (3800 76%) train loss: 1.3379, test_loss: 1.4814]\n",
      "What would I cannot make my wifes the Turks\n",
      "The centrate, 'tis here,\n",
      "Untime upon my sparing her reason \n",
      "\n",
      "[92m 34s (3900 78%) train loss: 1.3615, test_loss: 1.4762]\n",
      "Where in thy head in his wise and arms?\n",
      "\n",
      "BIRON:\n",
      "Reason inknowled me, and get you aid white marriage.\n",
      "\n",
      " \n",
      "\n",
      "[94m 56s (4000 80%) train loss: 1.3878, test_loss: 1.4614]\n",
      "Whererery: carro conveyed the contend with no longer sholy emperor and the French where he comes to se \n",
      "\n",
      "[97m 17s (4100 82%) train loss: 1.3499, test_loss: 1.4666]\n",
      "Where is the supply we down;\n",
      "But you make you dull can marries him 'hoped such a goodness it,\n",
      "But shir \n",
      "\n",
      "[99m 40s (4200 84%) train loss: 1.3712, test_loss: 1.4876]\n",
      "What things at his chil angry, to small breath the senses so such a subtless, and like rusby, thou art \n",
      "\n",
      "[102m 2s (4300 86%) train loss: 1.3618, test_loss: 1.5180]\n",
      "Who, nor you so following me: be compare the\n",
      "claming in brow in the deed\n",
      "must true,\n",
      "The king hath shou \n",
      "\n",
      "[104m 25s (4400 88%) train loss: 1.3446, test_loss: 1.4934]\n",
      "Who's for thee. Commend pricket way to himself:\n",
      "But not so.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "But that you convers indi \n",
      "\n",
      "[106m 48s (4500 90%) train loss: 1.3659, test_loss: 1.4814]\n",
      "Why would give you to the fault\n",
      "As so, make this comfort so linest brought you. So then I told you, Mo \n",
      "\n",
      "[109m 11s (4600 92%) train loss: 1.3453, test_loss: 1.4696]\n",
      "What you not well desire your half:\n",
      "There's not enter in the good of great graving men are upon me pri \n",
      "\n",
      "[111m 33s (4700 94%) train loss: 1.3422, test_loss: 1.4780]\n",
      "Where the field:\n",
      "My grace from your love from the numbing and little his present creature.\n",
      "It is not f \n",
      "\n",
      "[113m 55s (4800 96%) train loss: 1.3571, test_loss: 1.4686]\n",
      "Whom heavy sorrow,\n",
      "By thee lives:\n",
      "If you do take you good.\n",
      "\n",
      "DESDEMONA:\n",
      "So did the mean i' them\n",
      "taken m \n",
      "\n",
      "[116m 19s (4900 98%) train loss: 1.3579, test_loss: 1.4925]\n",
      "Why, the morning to the cleantual true?\n",
      "\n",
      "LAUNCE:\n",
      "Was depart is of a vausiness are heart:\n",
      "He go us; whe \n",
      "\n",
      "[118m 42s (5000 100%) train loss: 1.3373, test_loss: 1.4821]\n",
      "Whow you use the cause,\n",
      "not your kingdom we then all the time.\n",
      "\n",
      "AENEAS:\n",
      "Yes, sir, thine in Verre.\n",
      "\n",
      "FAL \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "# Adding scheduler for learning rate descent\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(rnn_optimizer, step_size=500, gamma=0.77)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "#     scheduler.step()\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network\n",
    "torch.save(rnn.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2298f444e0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "print(len(all_losses))\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou shalt be much to thee, Cain, such action to the death.\n",
      "\n",
      "JULIET:\n",
      "There now my master\n",
      "Made him better to thee in the ring:\n",
      "Adermen, let not this comes how their own age\n",
      "Stands to decains the senses\n",
      "With amiss.\n",
      "\n",
      "LUCIANA:\n",
      "There's not in all here:\n",
      "The rich entertainted her stand upon a followers,\n",
      "And show his art death; and all dark thou calm, that you are plays,\n",
      "The tremble in my coloers still.\n",
      "\n",
      "PRINCE HENRY:\n",
      "The prasking very next dream, sir.\n",
      "\n",
      "SIR, I think to longer made me well: speak the next of the death-day. But is the gods that begin my life.\n",
      "\n",
      "EDGAR:\n",
      "Here comes Ford is wronged limberland,\n",
      "Upon this talk of that song!\n",
      "\n",
      "GOBBO:\n",
      "This doth must be happiness,\n",
      "From her than to granted of me.\n",
      "\n",
      "ANTIPO:\n",
      "Thou dost news straight braggar in his courteous counce of brothers,\n",
      "Troth, you see no doubt, he is, of the wind to hear thy brothers in commend her self. What, can you shall be say in a present;\n",
      "We can have lose say without thee against her.\n",
      "\n",
      "YORK:\n",
      "I have shines of one hand of the lates. I \n"
     ]
    }
   ],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
